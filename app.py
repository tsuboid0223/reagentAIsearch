import streamlit as st
import requests
import google.generativeai as genai
import time
import re
import json
import pandas as pd
from urllib.parse import quote_plus, unquote, urlparse
from io import StringIO
from datetime import datetime
import random

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ§ª",
    layout="wide"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .product-card {
        background-color: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 0.5rem;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    .product-title {
        font-size: 1.3rem;
        font-weight: bold;
        color: #2c3e50;
        margin-bottom: 0.5rem;
    }
    .product-info {
        font-size: 1rem;
        color: #495057;
        margin: 0.3rem 0;
    }
    .price-row {
        background-color: white;
        padding: 0.8rem;
        margin: 0.3rem 0;
        border-radius: 0.3rem;
        border-left: 4px solid #007bff;
    }
    .log-container {
        background-color: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 0.5rem;
        padding: 1rem;
        font-family: monospace;
        font-size: 0.85rem;
        max-height: 400px;
        overflow-y: auto;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# User-Agent ãƒªã‚¹ãƒˆ
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
]

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ã‚¯ãƒ©ã‚¹
class RealTimeLogger:
    def __init__(self, container):
        self.container = container
        self.logs = []
        
    def log(self, message, level="INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"
        self.logs.append(log_entry)
        
        with self.container:
            st.code("\n".join(self.logs[-30:]), language="log")

# Gemini APIè¨­å®š
def setup_gemini():
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        return genai.GenerativeModel('gemini-2.0-flash')
    except Exception as e:
        st.error(f"âŒ Gemini APIè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

# å¯¾è±¡ECã‚µã‚¤ãƒˆã®å®šç¾©ï¼ˆ11ã‚µã‚¤ãƒˆï¼‰
TARGET_SITES = {
    "cosmobio": {"name": "ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª", "domain": "cosmobio.co.jp"},
    "funakoshi": {"name": "ãƒ•ãƒŠã‚³ã‚·", "domain": "funakoshi.co.jp"},
    "axel": {"name": "AXEL", "domain": "axel.as-1.co.jp"},
    "selleck": {"name": "Selleck", "domain": "selleck.co.jp"},
    "mce": {"name": "MCE", "domain": "medchemexpress.com"},
    "nakarai": {"name": "ãƒŠã‚«ãƒ©ã‚¤", "domain": "nacalai.co.jp"},
    "fujifilm": {"name": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ å’Œå…‰", "domain": "labchem-wako.fujifilm.com"},
    "kanto": {"name": "é–¢æ±åŒ–å­¦", "domain": "kanto.co.jp"},
    "tci": {"name": "TCI", "domain": "tcichemicals.com"},
    "merck": {"name": "Merck", "domain": "merck.com"},
    "wako": {"name": "å’Œå…‰ç´”è–¬", "domain": "hpc-j.co.jp"}
}

def clean_google_url(url):
    """Googleãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    try:
        # /url?q= å½¢å¼ã®å‡¦ç†
        if '/url?q=' in url or '/url?url=' in url:
            parsed = urlparse(url)
            from urllib.parse import parse_qs
            params = parse_qs(parsed.query)
            if 'q' in params:
                url = params['q'][0]
            elif 'url' in params:
                url = params['url'][0]
        
        # URLãƒ‡ã‚³ãƒ¼ãƒ‰
        url = unquote(url)
        
        # ä¸è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
        url = url.split('&sa=')[0].split('&ved=')[0]
        
        return url
    except:
        return url

def extract_urls_from_html_improved(html_content, domain):
    """æ”¹å–„ã•ã‚ŒãŸURLæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯"""
    
    # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è©¦è¡Œ
    patterns = [
        # æ¨™æº–çš„ãªURL
        rf'https?://(?:www\.)?{re.escape(domain)}[^\s<>"\'\)]*',
        # Googleãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå½¢å¼
        rf'/url\?q=https?://(?:www\.)?{re.escape(domain)}[^&\s<>"\']*',
        rf'/url\?url=https?://(?:www\.)?{re.escape(domain)}[^&\s<>"\']*',
        # hrefå±æ€§å†…
        rf'href="(https?://(?:www\.)?{re.escape(domain)}[^"]*)"',
        rf"href='(https?://(?:www\.)?{re.escape(domain)}[^']*)'",
    ]
    
    all_urls = set()
    
    for pattern in patterns:
        matches = re.findall(pattern, html_content, re.IGNORECASE)
        for match in matches:
            # ã‚¿ãƒ—ãƒ«ã®å ´åˆã¯æœ€åˆã®è¦ç´ ã‚’å–å¾—
            url = match[0] if isinstance(match, tuple) else match
            
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            url = clean_google_url(url)
            
            # æœ‰åŠ¹ãªURLã®ã¿è¿½åŠ 
            if url.startswith('http') and len(url) > 20:
                # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
                if not any(x in url.lower() for x in ['google.com', 'youtube.com', 'facebook.com', 'twitter.com']):
                    all_urls.add(url)
    
    # URLã®å„ªå…ˆé †ä½ä»˜ã‘
    priority_keywords = [
        'product', 'detail', 'item', 'price', 'catalog',
        'è£½å“', 'å•†å“', 'ä¾¡æ ¼', 'ã‚«ã‚¿ãƒ­ã‚°', 'p_view', 'view'
    ]
    
    prioritized = []
    others = []
    
    for url in all_urls:
        if any(keyword in url.lower() for keyword in priority_keywords):
            prioritized.append(url)
        else:
            others.append(url)
    
    result = (prioritized + others)[:10]
    
    return result

def search_with_retry(query, max_retries=3, logger=None):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãæ¤œç´¢"""
    
    for retry in range(max_retries):
        try:
            search_url = f"https://www.google.com/search?q={quote_plus(query)}&num=10"
            
            headers = {
                'User-Agent': random.choice(USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Referer': 'https://www.google.com/',
            }
            
            response = requests.get(
                search_url,
                headers=headers,
                timeout=15
            )
            
            if response.status_code == 200:
                if logger:
                    logger.log(f"  âœ“ æ¤œç´¢æˆåŠŸï¼ˆè©¦è¡Œ{retry+1}å›ç›®ï¼‰", "DEBUG")
                return response.text
            elif response.status_code == 429:
                if logger:
                    logger.log(f"  ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡ºã€å¾…æ©Ÿä¸­...", "WARNING")
                wait_time = (retry + 1) * 10
                time.sleep(wait_time)
            else:
                if logger:
                    logger.log(f"  HTTP {response.status_code}ã€ãƒªãƒˆãƒ©ã‚¤ä¸­...", "WARNING")
                time.sleep(random.uniform(3, 6))
                
        except Exception as e:
            if logger:
                logger.log(f"  æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}", "WARNING")
            if retry < max_retries - 1:
                time.sleep(random.uniform(5, 10))
    
    return None

def search_with_strategy(product_name, site_info, logger):
    """å¤šå±¤æˆ¦ç•¥ã§æ¤œç´¢ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    site_name = site_info["name"]
    domain = site_info["domain"]
    
    logger.log(f"ğŸ” {site_name}ã‚’æ¤œç´¢ä¸­", "INFO")
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³
    search_queries = [
        f"{product_name} ä¾¡æ ¼ site:{domain}",
        f"{product_name} site:{domain}",
        f"{product_name} ã‚«ã‚¿ãƒ­ã‚° site:{domain}",
    ]
    
    all_results = []
    
    for query_idx, query in enumerate(search_queries):
        logger.log(f"  æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³{query_idx+1}: {query[:60]}...", "DEBUG")
        
        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãæ¤œç´¢
        search_html = search_with_retry(query, max_retries=2, logger=logger)
        
        if not search_html:
            continue
        
        # æ”¹å–„ã•ã‚ŒãŸURLæŠ½å‡º
        urls = extract_urls_from_html_improved(search_html, domain)
        
        if urls:
            logger.log(f"  âœ“ {len(urls)}ä»¶ã®URLç™ºè¦‹", "INFO")
            
            for url in urls[:3]:
                all_results.append({
                    'url': url,
                    'site': site_name,
                    'search_html': search_html,
                    'query': query
                })
            
            # URLãŒè¦‹ã¤ã‹ã£ãŸã‚‰æ¬¡ã®ã‚µã‚¤ãƒˆã¸
            break
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        time.sleep(random.uniform(2, 4))
    
    if all_results:
        logger.log(f"âœ… {site_name}: {len(all_results)}ä»¶ã®URLå–å¾—", "INFO")
    else:
        logger.log(f"âš ï¸ {site_name}: URLæœªç™ºè¦‹", "WARNING")
    
    return all_results

def extract_price_from_search_snippet(search_html, product_name, model, logger):
    """æ¤œç´¢çµæœã‚¹ãƒ‹ãƒšãƒƒãƒˆã‹ã‚‰ä¾¡æ ¼æƒ…å ±ã‚’æŠ½å‡º"""
    logger.log(f"  ğŸ’¡ ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‹ã‚‰ä¾¡æ ¼æŠ½å‡ºä¸­", "DEBUG")
    
    try:
        search_html = search_html[:30000]
        
        prompt = f"""
ä»¥ä¸‹ã¯Googleæ¤œç´¢çµæœã®HTMLã§ã™ã€‚åŒ–å­¦è©¦è–¬ã€Œ{product_name}ã€ã«é–¢ã™ã‚‹ä¾¡æ ¼æƒ…å ±ã‚’ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‹ã‚‰æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€æŠ½å‡ºã™ã‚‹æƒ…å ±ã€‘
1. productName: è£½å“å
2. modelNumber: å‹ç•ªãƒ»ã‚«ã‚¿ãƒ­ã‚°ç•ªå·ï¼ˆã‚ã‚Œã°ï¼‰
3. manufacturer: ãƒ¡ãƒ¼ã‚«ãƒ¼åï¼ˆã‚ã‚Œã°ï¼‰
4. offers: ä¾¡æ ¼æƒ…å ±ã®ãƒªã‚¹ãƒˆ
   - size: å®¹é‡ãƒ»ã‚µã‚¤ã‚º
   - price: ä¾¡æ ¼ï¼ˆæ•°å€¤ã®ã¿ï¼‰
   - inStock: åœ¨åº«çŠ¶æ³ï¼ˆä¸æ˜ãªå ´åˆã¯trueï¼‰

ã€é‡è¦ã€‘
- ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚„ã‚¿ã‚¤ãƒˆãƒ«ã«ä¾¡æ ¼æƒ…å ±ï¼ˆÂ¥ã€å††ã€$ã€priceï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å¿…ãšæŠ½å‡º
- å‹ç•ªã¨ä¾¡æ ¼ãŒã‚»ãƒƒãƒˆã§è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å¯¾å¿œä»˜ã‘ã¦æŠ½å‡º
- ä¾¡æ ¼æƒ…å ±ãŒãªã„å ´åˆã¯offersã‚’ç©ºé…åˆ—ã«

ã€å‡ºåŠ›å½¢å¼ã€‘
JSONå½¢å¼ã®ã¿ã€‚ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ä¸è¦ã€‚

æ¤œç´¢çµæœHTML:
{search_html}
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        response_text = re.sub(r'^```json\s*', '', response_text)
        response_text = re.sub(r'^```\s*', '', response_text)
        response_text = re.sub(r'\s*```$', '', response_text)
        response_text = response_text.strip()
        
        price_info = json.loads(response_text)
        
        if price_info.get('offers'):
            logger.log(f"  âœ… ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‹ã‚‰{len(price_info['offers'])}ä»¶ã®ä¾¡æ ¼æŠ½å‡º", "INFO")
            return price_info
        else:
            logger.log(f"  â„¹ï¸ ã‚¹ãƒ‹ãƒšãƒƒãƒˆã«ä¾¡æ ¼æƒ…å ±ãªã—", "DEBUG")
            return None
            
    except Exception as e:
        logger.log(f"  ã‚¹ãƒ‹ãƒšãƒƒãƒˆè§£æã‚¨ãƒ©ãƒ¼: {str(e)}", "DEBUG")
        return None

def fetch_page_content(url, logger):
    """ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    try:
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8',
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            return response.text
    except:
        pass
    
    return None

def extract_product_info_from_page(html_content, product_name, model, logger):
    """ãƒšãƒ¼ã‚¸HTMLã‹ã‚‰è£½å“æƒ…å ±ã‚’æŠ½å‡º"""
    logger.log(f"  ğŸ¤– ãƒšãƒ¼ã‚¸å†…å®¹ã‚’AIåˆ†æä¸­", "DEBUG")
    
    try:
        html_content = html_content[:50000]
        
        prompt = f"""
ä»¥ä¸‹ã®HTMLã‹ã‚‰åŒ–å­¦è©¦è–¬ã€Œ{product_name}ã€ã®è£½å“æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€æŠ½å‡ºæƒ…å ±ã€‘
1. productName: è£½å“å
2. modelNumber: å‹ç•ª
3. manufacturer: ãƒ¡ãƒ¼ã‚«ãƒ¼å
4. offers: ä¾¡æ ¼æƒ…å ±
   - size, price, inStock

JSONå½¢å¼ã§å‡ºåŠ›ã€‚

HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„:
{html_content}
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        response_text = re.sub(r'^```json\s*', '', response_text)
        response_text = re.sub(r'^```\s*', '', response_text)
        response_text = re.sub(r'\s*```$', '', response_text)
        response_text = response_text.strip()
        
        product_info = json.loads(response_text)
        
        if product_info.get('offers'):
            logger.log(f"  âœ… ãƒšãƒ¼ã‚¸ã‹ã‚‰{len(product_info['offers'])}ä»¶ã®ä¾¡æ ¼æŠ½å‡º", "INFO")
        else:
            logger.log(f"  â„¹ï¸ ãƒšãƒ¼ã‚¸ã«ä¾¡æ ¼æƒ…å ±ãªã—", "DEBUG")
        
        return product_info
        
    except Exception as e:
        logger.log(f"  ãƒšãƒ¼ã‚¸è§£æã‚¨ãƒ©ãƒ¼: {str(e)}", "DEBUG")
        return None

def merge_product_info(snippet_info, page_info):
    """æƒ…å ±ã‚’ãƒãƒ¼ã‚¸"""
    if not snippet_info and not page_info:
        return None
    
    if not snippet_info:
        return page_info
    
    if not page_info:
        return snippet_info
    
    # ä¾¡æ ¼æƒ…å ±ãŒå¤šã„æ–¹ã‚’ãƒ™ãƒ¼ã‚¹ã«
    if len(snippet_info.get('offers', [])) >= len(page_info.get('offers', [])):
        merged = snippet_info.copy()
        if not merged.get('productName'):
            merged['productName'] = page_info.get('productName')
        if not merged.get('modelNumber'):
            merged['modelNumber'] = page_info.get('modelNumber')
        if not merged.get('manufacturer'):
            merged['manufacturer'] = page_info.get('manufacturer')
    else:
        merged = page_info.copy()
        if not merged.get('productName'):
            merged['productName'] = snippet_info.get('productName')
        if not merged.get('modelNumber'):
            merged['modelNumber'] = snippet_info.get('modelNumber')
        if not merged.get('manufacturer'):
            merged['manufacturer'] = snippet_info.get('manufacturer')
    
    return merged

def display_product_card(product, idx):
    """è£½å“æƒ…å ±ã‚’è¡¨ç¤º"""
    st.markdown(f'<div class="product-card">', unsafe_allow_html=True)
    
    product_name = product.get('productName', 'è£½å“åä¸æ˜')
    site_name = product.get('source_site', 'ä¸æ˜')
    st.markdown(f'<div class="product-title">ğŸ“¦ {product_name}</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown(f'<div class="product-info"><strong>è²©å£²å…ƒ:</strong> {site_name}</div>', unsafe_allow_html=True)
        st.markdown(f'<div class="product-info"><strong>å‹ç•ª:</strong> {product.get("modelNumber", "N/A")}</div>', unsafe_allow_html=True)
        st.markdown(f'<div class="product-info"><strong>ãƒ¡ãƒ¼ã‚«ãƒ¼:</strong> {product.get("manufacturer", "N/A")}</div>', unsafe_allow_html=True)
    
    with col2:
        source_url = product.get('source_url', '#')
        st.markdown(f'<div class="product-info"><strong>URL:</strong> <a href="{source_url}" target="_blank">è£½å“ãƒšãƒ¼ã‚¸ã‚’é–‹ã</a></div>', unsafe_allow_html=True)
    
    if 'offers' in product and product['offers']:
        st.markdown("**ğŸ’° ä¾¡æ ¼æƒ…å ±:**")
        
        for offer in product['offers']:
            size = offer.get('size', 'N/A')
            price = offer.get('price', 0)
            price_str = f"Â¥{price:,}" if price else 'N/A'
            stock = offer.get('inStock', False)
            stock_icon = "âœ…" if stock else "âŒ"
            stock_text = "åœ¨åº«ã‚ã‚Š" if stock else "åœ¨åº«ãªã—"
            
            st.markdown(
                f'<div class="price-row">'
                f'<strong>{size}</strong>: {price_str} {stock_icon} {stock_text}'
                f'</div>',
                unsafe_allow_html=True
            )
    else:
        st.warning("âš ï¸ ä¾¡æ ¼æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    st.markdown('</div>', unsafe_allow_html=True)

def main():
    st.markdown('<h1 class="main-header">ğŸ§ª åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆä¿®æ­£ç‰ˆï¼‰</h1>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        product_name = st.text_input(
            "ğŸ” è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value="Quinpirole",
            placeholder="ä¾‹: Y-27632, DMSO, Trizol, Quinpirole"
        )
    
    with col2:
        max_sites = st.number_input(
            "æœ€å¤§æ¤œç´¢ã‚µã‚¤ãƒˆæ•°",
            min_value=1,
            max_value=11,
            value=5,
            step=1
        )
    
    st.markdown("---")
    
    if st.button("ğŸš€ æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True):
        if not product_name:
            st.warning("âš ï¸ è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        st.markdown("### ğŸ“ å‡¦ç†ãƒ­ã‚°")
        log_container = st.empty()
        logger = RealTimeLogger(log_container)
        
        start_time = time.time()
        logger.log(f"ğŸš€ å‡¦ç†é–‹å§‹: {product_name}", "INFO")
        logger.log(f"ğŸ“Š æ”¹å–„: URLæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯å¼·åŒ–ã€ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½è¿½åŠ ", "INFO")
        
        model = setup_gemini()
        if not model:
            st.error("âŒ Gemini APIã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        all_products = []
        sites_to_search = dict(list(TARGET_SITES.items())[:max_sites])
        
        for site_key, site_info in sites_to_search.items():
            search_results = search_with_strategy(product_name, site_info, logger)
            
            if not search_results:
                time.sleep(random.uniform(2, 4))
                continue
            
            result = search_results[0]
            
            # ã‚¹ãƒ‹ãƒšãƒƒãƒˆåˆ†æ
            snippet_info = extract_price_from_search_snippet(
                result['search_html'],
                product_name,
                model,
                logger
            )
            
            # ãƒšãƒ¼ã‚¸åˆ†æ
            page_info = None
            html_content = fetch_page_content(result['url'], logger)
            if html_content:
                page_info = extract_product_info_from_page(html_content, product_name, model, logger)
            
            # ãƒãƒ¼ã‚¸
            merged_info = merge_product_info(snippet_info, page_info)
            
            if merged_info:
                merged_info['source_site'] = result['site']
                merged_info['source_url'] = result['url']
                all_products.append(merged_info)
                logger.log(f"âœ… {result['site']}: è£½å“æƒ…å ±å–å¾—æˆåŠŸ", "INFO")
            else:
                logger.log(f"âš ï¸ {result['site']}: è£½å“æƒ…å ±å–å¾—å¤±æ•—", "WARNING")
            
            time.sleep(random.uniform(3, 5))
        
        elapsed_time = time.time() - start_time
        logger.log(f"ğŸ‰ å‡¦ç†å®Œäº†: {elapsed_time:.1f}ç§’", "INFO")
        
        st.markdown("---")
        st.markdown("## ğŸ“‹ æ¤œç´¢çµæœ")
        
        if not all_products:
            st.warning("âš ï¸ è£½å“æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        with_price = [p for p in all_products if p.get('offers')]
        without_price = [p for p in all_products if not p.get('offers')]
        
        st.success(f"âœ… {len(all_products)}ä»¶ã®è£½å“æƒ…å ±ã‚’å–å¾—ï¼ˆä¾¡æ ¼æƒ…å ±ã‚ã‚Š: {len(with_price)}ä»¶ã€å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’ï¼‰")
        
        for idx, product in enumerate(with_price + without_price):
            display_product_card(product, idx)
        
        # CSVå‡ºåŠ›
        st.markdown("---")
        st.markdown("## ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        export_data = []
        for product in all_products:
            base_info = {
                'è£½å“å': product.get('productName', 'N/A'),
                'å‹ç•ª': product.get('modelNumber', 'N/A'),
                'ãƒ¡ãƒ¼ã‚«ãƒ¼': product.get('manufacturer', 'N/A'),
                'è²©å£²å…ƒ': product.get('source_site', 'N/A'),
                'URL': product.get('source_url', 'N/A')
            }
            
            if 'offers' in product and product['offers']:
                for offer in product['offers']:
                    row = base_info.copy()
                    row['ã‚µã‚¤ã‚º'] = offer.get('size', 'N/A')
                    row['ä¾¡æ ¼'] = offer.get('price', 0)
                    row['åœ¨åº«'] = 'æœ‰' if offer.get('inStock') else 'ç„¡'
                    export_data.append(row)
            else:
                row = base_info.copy()
                row['ã‚µã‚¤ã‚º'] = 'N/A'
                row['ä¾¡æ ¼'] = 0
                row['åœ¨åº«'] = 'N/A'
                export_data.append(row)
        
        df = pd.DataFrame(export_data)
        
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        csv_data = csv_buffer.getvalue()
        
        st.download_button(
            label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=f"chemical_prices_{product_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True
        )

if __name__ == "__main__":
    main()

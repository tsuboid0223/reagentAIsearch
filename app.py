import streamlit as st
import requests
import google.generativeai as genai
import time
import re
import json
import pandas as pd
from urllib.parse import quote_plus, urlparse, unquote
from io import StringIO
from datetime import datetime
import html

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆ403/404å¯¾ç­–ç‰ˆï¼‰",
    page_icon="ğŸ§ª",
    layout="wide"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .log-container {
        background-color: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 0.5rem;
        padding: 1rem;
        font-family: monospace;
        font-size: 0.85rem;
        max-height: 400px;
        overflow-y: auto;
        margin: 1rem 0;
    }
    .api-status {
        padding: 0.5rem 1rem;
        border-radius: 0.3rem;
        margin: 0.5rem 0;
        font-weight: bold;
    }
    .api-success {
        background-color: #d4edda;
        color: #155724;
        border: 1px solid #c3e6cb;
    }
    .api-warning {
        background-color: #fff3cd;
        color: #856404;
        border: 1px solid #ffeeba;
    }
</style>
""", unsafe_allow_html=True)

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ã‚¯ãƒ©ã‚¹
class RealTimeLogger:
    def __init__(self, container):
        self.container = container
        self.logs = []
        
    def log(self, message, level="INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"
        self.logs.append(log_entry)
        
        with self.container:
            st.code("\n".join(self.logs[-50:]), language="log")

# Gemini APIè¨­å®š
def setup_gemini():
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        return genai.GenerativeModel('gemini-2.0-flash')
    except Exception as e:
        st.error(f"âŒ Gemini APIè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

# SERP APIè¨­å®šãƒã‚§ãƒƒã‚¯
def check_serp_api_config():
    """SERP APIèªè¨¼æƒ…å ±ã®ç¢ºèª"""
    try:
        if "BRIGHTDATA_API_KEY" in st.secrets:
            zone_name = st.secrets.get("BRIGHTDATA_ZONE_NAME", "serp_api1")
            return {
                'provider': 'brightdata',
                'auth_type': 'api_key',
                'api_key': st.secrets["BRIGHTDATA_API_KEY"],
                'zone_name': zone_name,
                'available': True
            }
    except:
        pass
    
    return {
        'provider': None,
        'available': False
    }

# å¯¾è±¡ECã‚µã‚¤ãƒˆã®å®šç¾©ï¼ˆ11ã‚µã‚¤ãƒˆï¼‰
TARGET_SITES = {
    "cosmobio": {"name": "ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª", "domain": "cosmobio.co.jp"},
    "funakoshi": {"name": "ãƒ•ãƒŠã‚³ã‚·", "domain": "funakoshi.co.jp"},
    "axel": {"name": "AXEL", "domain": "axel.as-1.co.jp"},
    "selleck": {"name": "Selleck", "domain": "selleck.co.jp"},
    "mce": {"name": "MCE", "domain": "medchemexpress.com"},
    "nakarai": {"name": "ãƒŠã‚«ãƒ©ã‚¤", "domain": "nacalai.co.jp"},
    "fujifilm": {"name": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ å’Œå…‰", "domain": "labchem-wako.fujifilm.com"},
    "kanto": {"name": "é–¢æ±åŒ–å­¦", "domain": "kanto.co.jp"},
    "tci": {"name": "TCI", "domain": "tcichemicals.com"},
    "merck": {"name": "Merck", "domain": "merck.com"},
    "wako": {"name": "å’Œå…‰ç´”è–¬", "domain": "hpc-j.co.jp"}
}

def clean_url(url):
    """URLã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° - HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šé™¤"""
    try:
        # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆ&amp; â†’ &ï¼‰
        url = html.unescape(url)
        
        # URLãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆ%20 â†’ ã‚¹ãƒšãƒ¼ã‚¹ç­‰ï¼‰
        url = unquote(url)
        
        # URLã‚’ãƒ‘ãƒ¼ã‚¹
        parsed = urlparse(url)
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®Œå…¨ã«å‰Šé™¤
        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        
        # æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
        clean_url = clean_url.rstrip('/')
        
        # æœ«å°¾ã®è¨˜å·ã‚’å‰Šé™¤
        clean_url = clean_url.rstrip('.,;:)"\'')
        
        return clean_url
    except Exception as e:
        return url

def fetch_page_with_brightdata(url, serp_config, logger):
    """Bright DataçµŒç”±ã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆ403/404å¯¾ç­–ï¼‰"""
    try:
        # URLã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆé‡è¦ï¼ï¼‰
        clean_url_str = clean_url(url)
        logger.log(f"  ğŸŒ Bright DataçµŒç”±ã§ãƒšãƒ¼ã‚¸å–å¾—", "DEBUG")
        logger.log(f"    å…ƒURL: {url[:80]}...", "DEBUG")
        logger.log(f"    ã‚¯ãƒªãƒ¼ãƒ³URL: {clean_url_str[:80]}...", "DEBUG")
        
        api_url = "https://api.brightdata.com/request"
        
        headers = {
            'Authorization': f'Bearer {serp_config["api_key"]}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'zone': serp_config['zone_name'],
            'url': clean_url_str,  # ã‚¯ãƒªãƒ¼ãƒ³ãªURLã‚’ä½¿ç”¨
            'format': 'raw'
        }
        
        response = requests.post(api_url, headers=headers, json=payload, timeout=45)
        
        if response.status_code == 200:
            html_size = len(response.text)
            logger.log(f"  âœ… Bright Dataå¿œç­” (HTML: {html_size} chars)", "INFO")
            
            # HTMLã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆç•°å¸¸ã«å°ã•ã„å ´åˆã¯è­¦å‘Šï¼‰
            if html_size < 1000:
                logger.log(f"  âš ï¸ HTMLã‚µã‚¤ã‚ºãŒç•°å¸¸ã«å°ã•ã„ ({html_size} chars)", "WARNING")
                logger.log(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {response.text[:200]}", "DEBUG")
                return None  # ç›´æ¥HTTPã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
            return response.text
        else:
            logger.log(f"  âš ï¸ Bright Data HTTP {response.status_code}", "WARNING")
            logger.log(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text[:200]}", "DEBUG")
            return None
            
    except Exception as e:
        logger.log(f"  âŒ Bright Dataå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None

def fetch_page_direct(url, logger):
    """ç›´æ¥HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒšãƒ¼ã‚¸å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    try:
        # URLã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        clean_url_str = clean_url(url)
        logger.log(f"  ğŸ”„ ç›´æ¥HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰", "DEBUG")
        logger.log(f"    URL: {clean_url_str[:80]}...", "DEBUG")
        
        # ã‚ˆã‚Šè©³ç´°ãªãƒ˜ãƒƒãƒ€ãƒ¼
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://www.google.com/'
        }
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨
        session = requests.Session()
        response = session.get(clean_url_str, headers=headers, timeout=15, allow_redirects=True)
        
        if response.status_code == 200:
            html_size = len(response.text)
            logger.log(f"  âœ… ç›´æ¥å–å¾—æˆåŠŸ (HTML: {html_size} chars)", "INFO")
            
            # HTMLã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            if html_size < 1000:
                logger.log(f"  âš ï¸ HTMLã‚µã‚¤ã‚ºãŒç•°å¸¸ã«å°ã•ã„ ({html_size} chars)", "WARNING")
                logger.log(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text[:200]}", "DEBUG")
                return None
            
            return response.text
        else:
            logger.log(f"  âš ï¸ HTTP {response.status_code}", "WARNING")
            return None
            
    except Exception as e:
        logger.log(f"  âŒ ç›´æ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None

def search_with_brightdata_serp(query, serp_config, logger):
    """Bright Data SERP APIã§æ¤œç´¢"""
    try:
        api_url = "https://api.brightdata.com/request"
        search_url = f"https://www.google.com/search?q={quote_plus(query)}&num=10&hl=ja&gl=jp"
        
        headers = {
            'Authorization': f'Bearer {serp_config["api_key"]}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'zone': serp_config['zone_name'],
            'url': search_url,
            'format': 'raw'
        }
        
        response = requests.post(api_url, headers=headers, json=payload, timeout=30)
        
        if response.status_code == 200:
            logger.log(f"  âœ“ SERP APIå¿œç­”æˆåŠŸ (HTML: {len(response.text)} chars)", "DEBUG")
            return {'html': response.text, 'status': 'success'}
        else:
            logger.log(f"  âš ï¸ SERP API HTTP {response.status_code}", "WARNING")
            return None
            
    except Exception as e:
        logger.log(f"  âŒ SERP APIã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None

def extract_urls_from_html(html_content, domain, logger):
    """HTMLã‹ã‚‰URLã‚’æŠ½å‡ºï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰"""
    urls = []
    
    try:
        patterns = [
            rf'href=["\']?(https?://(?:www\.)?{re.escape(domain)}[^"\'\s>]*)["\']?',
            rf'(https?://(?:www\.)?{re.escape(domain)}[^\s<>"\'()]*)',
            rf'location\.href\s*=\s*["\']?(https?://(?:www\.)?{re.escape(domain)}[^"\'\s]*)["\']?',
        ]
        
        all_urls = set()
        
        for pattern_idx, pattern in enumerate(patterns):
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            
            for match in matches:
                url = match[0] if isinstance(match, tuple) else match
                
                # URLã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                url = clean_url(url)
                
                # æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                if url.startswith('http') and len(url) > 20:
                    exclude_patterns = ['google.com', 'youtube.com', 'facebook.com', 
                                       'twitter.com', 'linkedin.com', '/search', '/tag/', 
                                       'translate.google', 'webcache.googleusercontent']
                    if not any(ex in url.lower() for ex in exclude_patterns):
                        all_urls.add(url)
        
        logger.log(f"    åˆè¨ˆ {len(all_urls)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯URLç™ºè¦‹", "DEBUG")
        
        # URLå“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scored_urls = []
        for url in all_urls:
            score = 0
            url_lower = url.lower()
            
            # è£½å“ãƒšãƒ¼ã‚¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            if any(kw in url_lower for kw in ['product', 'item', 'goods', 'detail', 'catalog', 'contents']):
                score += 10
            # æ•°å­—ã‚’å«ã‚€ï¼ˆè£½å“ã‚³ãƒ¼ãƒ‰ï¼‰
            if re.search(r'\d{3,}', url):
                score += 5
            # çŸ­ã™ãã‚‹URLã¯æ¸›ç‚¹
            if len(url) < 40:
                score -= 5
            
            scored_urls.append((url, score))
        
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        scored_urls.sort(key=lambda x: x[1], reverse=True)
        
        # ä¸Šä½10ä»¶ã‚’è¿”ã™
        for url, score in scored_urls[:10]:
            urls.append({
                'url': url,
                'title': '',
                'snippet': '',
                'score': score
            })
            logger.log(f"    âœ“ URL: {url[:80]}... (ã‚¹ã‚³ã‚¢:{score})", "DEBUG")
        
        if urls:
            logger.log(f"  âœ… {len(urls)}ä»¶ã®URLæŠ½å‡ºæˆåŠŸ", "INFO")
        else:
            logger.log(f"  âš ï¸ è©²å½“URLãªã—", "WARNING")
        
        return urls
        
    except Exception as e:
        logger.log(f"  âŒ URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return []

def search_with_strategy(product_name, site_info, serp_config, logger):
    """SERP APIã‚’ä½¿ç”¨ã—ãŸæ¤œç´¢æˆ¦ç•¥"""
    site_name = site_info["name"]
    domain = site_info["domain"]
    
    logger.log(f"ğŸ” {site_name} ({domain})ã‚’æ¤œç´¢ä¸­", "INFO")
    
    if not serp_config['available']:
        logger.log(f"  âŒ SERP APIæœªè¨­å®š", "ERROR")
        return []
    
    search_queries = [
        f"{product_name} site:{domain}",
        f"{product_name} price site:{domain}",
        f"{product_name} ä¾¡æ ¼ site:{domain}",
        f"{product_name} product site:{domain}",
        f"{product_name} ã‚«ã‚¿ãƒ­ã‚° site:{domain}",
    ]
    
    all_results = []
    
    for query_idx, query in enumerate(search_queries):
        logger.log(f"  ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª{query_idx+1}/5: {query}", "DEBUG")
        
        serp_data = search_with_brightdata_serp(query, serp_config, logger)
        
        if not serp_data:
            time.sleep(1)
            continue
        
        urls = extract_urls_from_html(serp_data['html'], domain, logger)
        
        if urls:
            for url_data in urls[:5]:
                all_results.append({
                    'url': url_data['url'],
                    'site': site_name,
                    'title': url_data.get('title', ''),
                    'snippet': url_data.get('snippet', ''),
                    'score': url_data.get('score', 0)
                })
            
            logger.log(f"  âœ… {len(urls)}ä»¶ã®URLå–å¾—æˆåŠŸ", "INFO")
            break
        
        time.sleep(1)
    
    if all_results:
        logger.log(f"âœ… {site_name}: {len(all_results)}ä»¶ã®URLå–å¾—", "INFO")
    else:
        logger.log(f"âŒ {site_name}: URLæœªç™ºè¦‹", "ERROR")
    
    return all_results

def extract_product_info_from_page(html_content, product_name, url, model, logger):
    """ãƒšãƒ¼ã‚¸HTMLã‹ã‚‰è£½å“æƒ…å ±ã‚’æŠ½å‡º"""
    logger.log(f"  ğŸ¤– Gemini AIã§è£½å“æƒ…å ±ã‚’æŠ½å‡ºä¸­...", "DEBUG")
    
    try:
        html_content = html_content[:100000]
        
        prompt = f"""
ã‚ãªãŸã¯åŒ–å­¦è©¦è–¬ã®è£½å“æƒ…å ±æŠ½å‡ºã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®HTMLã‹ã‚‰ã€Œ{product_name}ã€ã®è£½å“æƒ…å ±ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
1. HTMLã‹ã‚‰ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡º:
   - productName: è£½å“åï¼ˆæ–‡å­—åˆ—ï¼‰
   - modelNumber: å‹ç•ªã€CASç•ªå·ã€è£½å“ã‚³ãƒ¼ãƒ‰ï¼ˆæ–‡å­—åˆ—ï¼‰
   - manufacturer: ãƒ¡ãƒ¼ã‚«ãƒ¼åï¼ˆæ–‡å­—åˆ—ï¼‰
   - offers: ä¾¡æ ¼æƒ…å ±ã®é…åˆ—

2. offersé…åˆ—ã®å„è¦ç´ :
   - size: å®¹é‡ãƒ»ã‚µã‚¤ã‚ºï¼ˆä¾‹: "1mg", "5mg", "10mL"ï¼‰
   - price: ä¾¡æ ¼ï¼ˆå¿…ãšæ•°å€¤å‹ã€ã‚«ãƒ³ãƒãªã—æ•´æ•°ã¾ãŸã¯å°æ•°ï¼‰
   - inStock: åœ¨åº«çŠ¶æ³ï¼ˆçœŸå½å€¤: true/falseï¼‰

3. ä¾¡æ ¼ã®æŠ½å‡ºè¦å‰‡:
   - ã€ŒÂ¥34,000ã€â†’ 34000
   - ã€Œ34,000å††ã€â†’ 34000
   - ã€Œ$340.00ã€â†’ 340
   - ã€Œâ‚¬250.00ã€â†’ 250
   - ã€Œç¨æŠœ Â¥32,000ã€â†’ 32000
   - ä¾¡æ ¼ãŒãªã„å ´åˆã¯ offers ã‚’ç©ºé…åˆ— [] ã«ã™ã‚‹

4. åœ¨åº«æƒ…å ±:
   - ã€Œåœ¨åº«ã‚ã‚Šã€ã€ŒIn Stockã€ã€ŒAvailableã€â†’ true
   - ã€Œåœ¨åº«ãªã—ã€ã€ŒOut of Stockã€ã€Œå“åˆ‡ã‚Œã€â†’ false
   - ä¸æ˜ãªå ´åˆã¯ true

5. å‡ºåŠ›å½¢å¼: å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ã€å‡ºåŠ›ä¾‹ã€‘
{{
  "productName": "Y-27632 dihydrochloride",
  "modelNumber": "146986-50-7",
  "manufacturer": "Sigma-Aldrich",
  "offers": [
    {{"size": "1mg", "price": 34000, "inStock": true}},
    {{"size": "5mg", "price": 54000, "inStock": true}},
    {{"size": "25mg", "price": 164000, "inStock": false}}
  ]
}}

ã€HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
{html_content}

ã€ã‚½ãƒ¼ã‚¹URLã€‘
{url}

å¿…ãšJSONå½¢å¼ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        logger.log(f"  ğŸ“¨ Gemini APIå¿œç­”å—ä¿¡ ({len(response_text)} chars)", "DEBUG")
        
        # JSONãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        response_text = re.sub(r'^```json\s*', '', response_text)
        response_text = re.sub(r'^```\s*', '', response_text)
        response_text = re.sub(r'\s*```$', '', response_text)
        response_text = response_text.strip()
        
        # JSONãƒ‘ãƒ¼ã‚¹
        product_info = json.loads(response_text)
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼ã¨ä¿®æ­£
        if 'offers' in product_info and isinstance(product_info['offers'], list):
            valid_offers = []
            for offer in product_info['offers']:
                if 'price' in offer:
                    try:
                        if isinstance(offer['price'], str):
                            price_str = offer['price'].replace(',', '').replace('Â¥', '').replace('å††', '').replace('$', '').replace('â‚¬', '').strip()
                            offer['price'] = float(price_str)
                        else:
                            offer['price'] = float(offer['price'])
                        
                        if offer['price'] > 0:
                            valid_offers.append(offer)
                    except:
                        logger.log(f"    âš ï¸ ä¾¡æ ¼å¤‰æ›ã‚¨ãƒ©ãƒ¼: {offer.get('price')}", "DEBUG")
            
            product_info['offers'] = valid_offers
        
        if product_info.get('offers'):
            logger.log(f"  âœ… {len(product_info['offers'])}ä»¶ã®ä¾¡æ ¼æƒ…å ±ã‚’æŠ½å‡º", "INFO")
            for i, offer in enumerate(product_info['offers'][:3]):
                logger.log(f"    - {offer.get('size', 'N/A')}: Â¥{int(offer.get('price', 0)):,}", "DEBUG")
        else:
            logger.log(f"  âš ï¸ ä¾¡æ ¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
        
        return product_info
        
    except json.JSONDecodeError as e:
        logger.log(f"  âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        logger.log(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_text[:300]}...", "DEBUG")
        return None
    except Exception as e:
        logger.log(f"  âŒ è£½å“æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None

def main():
    st.markdown('<h1 class="main-header">ğŸ§ª åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆ403/404å¯¾ç­–ç‰ˆï¼‰</h1>', unsafe_allow_html=True)
    
    serp_config = check_serp_api_config()
    
    if serp_config['available']:
        st.markdown(
            f'<div class="api-status api-success">âœ… SERP APIæ¥ç¶š: BRIGHTDATA (Zone: {serp_config["zone_name"]})</div>',
            unsafe_allow_html=True
        )
    else:
        st.markdown(
            '<div class="api-status api-warning">âš ï¸ SERP APIæœªè¨­å®š</div>',
            unsafe_allow_html=True
        )
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        product_name = st.text_input(
            "ğŸ” è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value="Y-27632",
            placeholder="ä¾‹: Y-27632, DMSO, Trizol, Quinpirole"
        )
    
    with col2:
        max_sites = st.number_input(
            "æœ€å¤§æ¤œç´¢ã‚µã‚¤ãƒˆæ•°",
            min_value=1,
            max_value=11,
            value=11,
            step=1
        )
    
    st.markdown("---")
    
    search_disabled = not serp_config['available']
    
    if st.button("ğŸš€ æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True, disabled=search_disabled):
        if not product_name:
            st.warning("âš ï¸ è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        st.markdown("### ğŸ“ å‡¦ç†ãƒ­ã‚°")
        log_container = st.empty()
        logger = RealTimeLogger(log_container)
        
        start_time = time.time()
        logger.log(f"ğŸš€ å‡¦ç†é–‹å§‹: {product_name}", "INFO")
        logger.log(f"ğŸ”Œ SERP API: BRIGHTDATA (Zone: {serp_config['zone_name']})", "INFO")
        logger.log(f"ğŸ¯ å¯¾è±¡ã‚µã‚¤ãƒˆæ•°: {max_sites}ã‚µã‚¤ãƒˆ", "INFO")
        logger.log(f"ğŸ›¡ï¸ 403/404å¯¾ç­–: Bright DataçµŒç”±ãƒšãƒ¼ã‚¸å–å¾— + ç›´æ¥HTTPãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯", "INFO")
        
        model = setup_gemini()
        if not model:
            st.error("âŒ Gemini APIã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        all_products = []
        sites_to_search = dict(list(TARGET_SITES.items())[:max_sites])
        
        for site_idx, (site_key, site_info) in enumerate(sites_to_search.items(), 1):
            logger.log(f"\n--- ã‚µã‚¤ãƒˆ {site_idx}/{max_sites} ---", "INFO")
            
            search_results = search_with_strategy(product_name, site_info, serp_config, logger)
            
            if not search_results:
                logger.log(f"â­ï¸  æ¬¡ã®ã‚µã‚¤ãƒˆã¸", "DEBUG")
                time.sleep(2)
                continue
            
            # æœ€ã‚‚ã‚¹ã‚³ã‚¢ãŒé«˜ã„URLã‚’ä½¿ç”¨
            search_results.sort(key=lambda x: x.get('score', 0), reverse=True)
            result = search_results[0]
            
            logger.log(f"ğŸ¯ ãƒˆãƒƒãƒ—URL: {result['url'][:80]}...", "INFO")
            
            # ãƒšãƒ¼ã‚¸å–å¾—æˆ¦ç•¥: 1. Bright DataçµŒç”± â†’ 2. ç›´æ¥HTTP
            html_content = None
            
            # æˆ¦ç•¥1: Bright DataçµŒç”±ã§å–å¾—
            html_content = fetch_page_with_brightdata(result['url'], serp_config, logger)
            
            # æˆ¦ç•¥2: å¤±æ•—ã—ãŸå ´åˆã¯ç›´æ¥HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            if not html_content:
                logger.log(f"  ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ", "INFO")
                html_content = fetch_page_direct(result['url'], logger)
            
            if html_content:
                page_info = extract_product_info_from_page(html_content, product_name, result['url'], model, logger)
                
                if page_info:
                    page_info['source_site'] = result['site']
                    page_info['source_url'] = result['url']
                    all_products.append(page_info)
                    logger.log(f"âœ… {result['site']}: è£½å“æƒ…å ±å–å¾—æˆåŠŸ", "INFO")
                else:
                    logger.log(f"âš ï¸ {result['site']}: AIè§£æå¤±æ•—", "WARNING")
            else:
                logger.log(f"âŒ {result['site']}: ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—ï¼ˆå…¨æˆ¦ç•¥å¤±æ•—ï¼‰", "ERROR")
            
            time.sleep(2)
        
        elapsed_time = time.time() - start_time
        logger.log(f"\nğŸ‰ å‡¦ç†å®Œäº†: {elapsed_time:.1f}ç§’", "INFO")
        logger.log(f"ğŸ“Š å–å¾—æˆåŠŸ: {len(all_products)}/{max_sites}ã‚µã‚¤ãƒˆ", "INFO")
        
        st.markdown("---")
        st.markdown("## ğŸ“‹ æ¤œç´¢çµæœ")
        
        if not all_products:
            st.error("âŒ è£½å“æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: è£½å“åã‚’å¤‰æ›´ã™ã‚‹ã‹ã€æ¤œç´¢å¯¾è±¡ã‚µã‚¤ãƒˆã‚’èª¿æ•´ã—ã¦ãã ã•ã„")
            return
        
        with_price = [p for p in all_products if p.get('offers')]
        without_price = [p for p in all_products if not p.get('offers')]
        
        st.success(f"âœ… {len(all_products)}ä»¶ã®è£½å“æƒ…å ±ã‚’å–å¾—ï¼ˆä¾¡æ ¼æƒ…å ±ã‚ã‚Š: {len(with_price)}ä»¶ã€å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’ï¼‰")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¡¨ç¤º
        table_data = []
        for product in all_products:
            base_info = {
                'è£½å“å': product.get('productName', 'N/A'),
                'è²©å£²å…ƒ': product.get('source_site', 'N/A'),
                'å‹ç•ª': product.get('modelNumber', 'N/A'),
                'ãƒ¡ãƒ¼ã‚«ãƒ¼': product.get('manufacturer', 'N/A')
            }
            
            if 'offers' in product and product['offers']:
                for offer in product['offers']:
                    row = base_info.copy()
                    row['å®¹é‡'] = offer.get('size', 'N/A')
                    
                    try:
                        price = offer.get('price', 0)
                        if isinstance(price, (int, float)) and price > 0:
                            row['ä¾¡æ ¼'] = f"Â¥{int(price):,}"
                        else:
                            row['ä¾¡æ ¼'] = 'N/A'
                    except:
                        row['ä¾¡æ ¼'] = 'N/A'
                    
                    row['åœ¨åº«æœ‰ç„¡'] = 'æœ‰' if offer.get('inStock') else 'ç„¡'
                    table_data.append(row)
            else:
                row = base_info.copy()
                row['å®¹é‡'] = 'N/A'
                row['ä¾¡æ ¼'] = 'N/A'
                row['åœ¨åº«æœ‰ç„¡'] = 'N/A'
                table_data.append(row)
        
        if table_data:
            df_display = pd.DataFrame(table_data)
            st.dataframe(df_display, use_container_width=True, height=600)
        
        # CSVå‡ºåŠ›
        st.markdown("---")
        st.markdown("## ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        export_data = []
        for product in all_products:
            base_info = {
                'è£½å“å': product.get('productName', 'N/A'),
                'å‹ç•ª': product.get('modelNumber', 'N/A'),
                'ãƒ¡ãƒ¼ã‚«ãƒ¼': product.get('manufacturer', 'N/A'),
                'è²©å£²å…ƒ': product.get('source_site', 'N/A'),
                'URL': product.get('source_url', 'N/A')
            }
            
            if 'offers' in product and product['offers']:
                for offer in product['offers']:
                    row = base_info.copy()
                    row['ã‚µã‚¤ã‚º'] = offer.get('size', 'N/A')
                    
                    try:
                        price = offer.get('price', 0)
                        row['ä¾¡æ ¼'] = int(price) if isinstance(price, (int, float)) else 0
                    except:
                        row['ä¾¡æ ¼'] = 0
                    
                    row['åœ¨åº«'] = 'æœ‰' if offer.get('inStock') else 'ç„¡'
                    export_data.append(row)
            else:
                row = base_info.copy()
                row['ã‚µã‚¤ã‚º'] = 'N/A'
                row['ä¾¡æ ¼'] = 0
                row['åœ¨åº«'] = 'N/A'
                export_data.append(row)
        
        df = pd.DataFrame(export_data)
        
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        csv_data = csv_buffer.getvalue()
        
        st.download_button(
            label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=f"chemical_prices_{product_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True
        )

if __name__ == "__main__":
    main()

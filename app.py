import streamlit as st
import google.generativeai as genai
import time
import re
import json
import pandas as pd
from io import StringIO
from datetime import datetime
from playwright.sync_api import sync_playwright
import urllib.parse

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆBrowser APIç‰ˆï¼‰",
    page_icon="ğŸ§ª",
    layout="wide"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .api-status {
        padding: 0.5rem 1rem;
        border-radius: 0.3rem;
        margin: 0.5rem 0;
        font-weight: bold;
    }
    .api-success {
        background-color: #d4edda;
        color: #155724;
        border: 1px solid #c3e6cb;
    }
</style>
""", unsafe_allow_html=True)

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ã‚¯ãƒ©ã‚¹
class RealTimeLogger:
    def __init__(self, container):
        self.container = container
        self.logs = []
        
    def log(self, message, level="INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"
        self.logs.append(log_entry)
        
        with self.container:
            st.code("\n".join(self.logs[-50:]), language="log")

# Gemini APIè¨­å®š
def setup_gemini():
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        return genai.GenerativeModel('gemini-2.0-flash')
    except Exception as e:
        st.error(f"âŒ Gemini APIè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

# Browser APIè¨­å®š
BROWSER_API_CONFIG = {
    'ws_endpoint': 'wss://brd-customer-hl_3c49a4bb-zone-scraping_browser1:lokq2uz6vn5q@brd.superproxy.io:9222',
    'available': True
}

# å¯¾è±¡ECã‚µã‚¤ãƒˆã®å®šç¾©ï¼ˆ11ã‚µã‚¤ãƒˆï¼‰
TARGET_SITES = {
    "cosmobio": {"name": "ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª", "domain": "cosmobio.co.jp"},
    "funakoshi": {"name": "ãƒ•ãƒŠã‚³ã‚·", "domain": "funakoshi.co.jp"},
    "axel": {"name": "AXEL", "domain": "axel.as-1.co.jp"},
    "selleck": {"name": "Selleck", "domain": "selleck.co.jp"},
    "mce": {"name": "MCE", "domain": "medchemexpress.com"},
    "nakarai": {"name": "ãƒŠã‚«ãƒ©ã‚¤", "domain": "nacalai.co.jp"},
    "fujifilm": {"name": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ å’Œå…‰", "domain": "labchem-wako.fujifilm.com"},
    "kanto": {"name": "é–¢æ±åŒ–å­¦", "domain": "kanto.co.jp"},
    "tci": {"name": "TCI", "domain": "tcichemicals.com"},
    "merck": {"name": "Merck", "domain": "merck.com"},
    "wako": {"name": "å’Œå…‰ç´”è–¬", "domain": "hpc-j.co.jp"}
}

def search_google_with_browser(query, logger):
    """Browser APIçµŒç”±ã§Googleæ¤œç´¢ã‚’å®Ÿè¡Œ"""
    try:
        logger.log(f"  ğŸ” Googleæ¤œç´¢: {query[:60]}...", "DEBUG")
        
        search_url = f"https://www.google.com/search?q={urllib.parse.quote_plus(query)}&num=10&hl=ja"
        
        with sync_playwright() as p:
            browser = p.chromium.connect_over_cdp(BROWSER_API_CONFIG['ws_endpoint'])
            context = browser.contexts[0]
            page = context.new_page()
            
            page.goto(search_url, timeout=30000, wait_until='domcontentloaded')
            time.sleep(2)  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            
            html_content = page.content()
            
            page.close()
            browser.close()
            
            logger.log(f"  âœ… Googleæ¤œç´¢æˆåŠŸ (HTML: {len(html_content)} chars)", "DEBUG")
            return html_content
            
    except Exception as e:
        logger.log(f"  âŒ Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None

def extract_urls_from_html(html_content, domain, logger):
    """HTMLã‹ã‚‰URLã‚’æŠ½å‡º"""
    urls = []
    
    try:
        patterns = [
            rf'href=["\']?(https?://(?:www\.)?{re.escape(domain)}[^"\'\s>]*)["\']?',
            rf'(https?://(?:www\.)?{re.escape(domain)}[^\s<>"\'()]*)',
        ]
        
        all_urls = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            
            for match in matches:
                url = match[0] if isinstance(match, tuple) else match
                
                # URLã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                # Googleãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                if '&ved=' in url:
                    url = url.split('&ved=')[0]
                elif '?ved=' in url:
                    url = url.split('?ved=')[0]
                
                # ãã®ä»–ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                for param in ['&hl=', '?hl=', '&sl=', '&tl=', '&client=']:
                    if param in url:
                        url = url.split(param)[0]
                
                # æœ«å°¾ã®è¨˜å·å‰Šé™¤
                url = url.rstrip('.,;:)"\'')
                
                # æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                if url.startswith('http') and len(url) > 20:
                    exclude_patterns = ['google.com', 'youtube.com', 'translate.google', 'webcache']
                    if not any(ex in url.lower() for ex in exclude_patterns):
                        all_urls.add(url)
        
        logger.log(f"    åˆè¨ˆ {len(all_urls)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯URLç™ºè¦‹", "DEBUG")
        
        # URLå“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scored_urls = []
        for url in all_urls:
            score = 0
            url_lower = url.lower()
            
            if any(kw in url_lower for kw in ['product', 'item', 'detail', 'catalog', 'contents']):
                score += 10
            if re.search(r'\d{3,}', url):
                score += 5
            
            scored_urls.append((url, score))
        
        scored_urls.sort(key=lambda x: x[1], reverse=True)
        
        for url, score in scored_urls[:10]:
            urls.append({
                'url': url,
                'score': score
            })
            logger.log(f"    âœ“ URL (ã‚¹ã‚³ã‚¢:{score}): {url[:80]}...", "DEBUG")
        
        if urls:
            logger.log(f"  âœ… {len(urls)}ä»¶ã®URLæŠ½å‡ºæˆåŠŸ", "INFO")
        else:
            logger.log(f"  âš ï¸ è©²å½“URLãªã—", "WARNING")
        
        return urls
        
    except Exception as e:
        logger.log(f"  âŒ URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return []

def fetch_page_with_browser(url, logger):
    """Browser APIçµŒç”±ã§ãƒšãƒ¼ã‚¸å–å¾—"""
    try:
        logger.log(f"  ğŸŒ Browser APIçµŒç”±ã§ãƒšãƒ¼ã‚¸å–å¾—", "DEBUG")
        logger.log(f"    URL: {url[:80]}...", "DEBUG")
        
        with sync_playwright() as p:
            browser = p.chromium.connect_over_cdp(BROWSER_API_CONFIG['ws_endpoint'])
            context = browser.contexts[0]
            page = context.new_page()
            
            # ãƒšãƒ¼ã‚¸ã«ç§»å‹•
            page.goto(url, timeout=45000, wait_until='networkidle')
            time.sleep(2)  # è¿½åŠ ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            
            # HTMLã‚’å–å¾—
            html_content = page.content()
            html_size = len(html_content)
            
            page.close()
            browser.close()
            
            logger.log(f"  âœ… ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ (HTML: {html_size} chars)", "INFO")
            
            # HTMLã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            if html_size < 1000:
                logger.log(f"  âš ï¸ HTMLã‚µã‚¤ã‚ºãŒç•°å¸¸ã«å°ã•ã„ ({html_size} chars)", "WARNING")
                return None
            
            return html_content
            
    except Exception as e:
        logger.log(f"  âŒ Browser APIå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None

def search_with_strategy(product_name, site_info, logger):
    """æ¤œç´¢æˆ¦ç•¥"""
    site_name = site_info["name"]
    domain = site_info["domain"]
    
    logger.log(f"ğŸ” {site_name} ({domain})ã‚’æ¤œç´¢ä¸­", "INFO")
    
    search_queries = [
        f"{product_name} site:{domain}",
        f"{product_name} price site:{domain}",
        f"{product_name} ä¾¡æ ¼ site:{domain}",
    ]
    
    all_results = []
    
    for query_idx, query in enumerate(search_queries):
        logger.log(f"  ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª{query_idx+1}/3: {query}", "DEBUG")
        
        html = search_google_with_browser(query, logger)
        
        if not html:
            time.sleep(2)
            continue
        
        urls = extract_urls_from_html(html, domain, logger)
        
        if urls:
            for url_data in urls[:5]:
                all_results.append({
                    'url': url_data['url'],
                    'site': site_name,
                    'score': url_data.get('score', 0)
                })
            
            logger.log(f"  âœ… {len(urls)}ä»¶ã®URLå–å¾—æˆåŠŸ", "INFO")
            break
        
        time.sleep(2)
    
    if all_results:
        logger.log(f"âœ… {site_name}: {len(all_results)}ä»¶ã®URLå–å¾—", "INFO")
    else:
        logger.log(f"âŒ {site_name}: URLæœªç™ºè¦‹", "ERROR")
    
    return all_results

def extract_product_info_from_page(html_content, product_name, url, model, logger):
    """ãƒšãƒ¼ã‚¸HTMLã‹ã‚‰è£½å“æƒ…å ±ã‚’æŠ½å‡º"""
    logger.log(f"  ğŸ¤– Gemini AIã§è£½å“æƒ…å ±ã‚’æŠ½å‡ºä¸­...", "DEBUG")
    
    try:
        html_content = html_content[:100000]
        
        prompt = f"""
ã‚ãªãŸã¯åŒ–å­¦è©¦è–¬ã®è£½å“æƒ…å ±æŠ½å‡ºã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®HTMLã‹ã‚‰ã€Œ{product_name}ã€ã®è£½å“æƒ…å ±ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
1. HTMLã‹ã‚‰ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡º:
   - productName: è£½å“åï¼ˆæ–‡å­—åˆ—ï¼‰
   - modelNumber: å‹ç•ªã€CASç•ªå·ã€è£½å“ã‚³ãƒ¼ãƒ‰ï¼ˆæ–‡å­—åˆ—ï¼‰
   - manufacturer: ãƒ¡ãƒ¼ã‚«ãƒ¼åï¼ˆæ–‡å­—åˆ—ï¼‰
   - offers: ä¾¡æ ¼æƒ…å ±ã®é…åˆ—

2. offersé…åˆ—ã®å„è¦ç´ :
   - size: å®¹é‡ãƒ»ã‚µã‚¤ã‚ºï¼ˆä¾‹: "1mg", "5mg", "10mL"ï¼‰
   - price: ä¾¡æ ¼ï¼ˆå¿…ãšæ•°å€¤å‹ã€ã‚«ãƒ³ãƒãªã—æ•´æ•°ã¾ãŸã¯å°æ•°ï¼‰
   - inStock: åœ¨åº«çŠ¶æ³ï¼ˆçœŸå½å€¤: true/falseï¼‰

3. ä¾¡æ ¼ã®æŠ½å‡ºè¦å‰‡:
   - ã€ŒÂ¥34,000ã€â†’ 34000
   - ã€Œ34,000å††ã€â†’ 34000
   - ã€Œ$340.00ã€â†’ 340
   - ã€Œç¨æŠœ Â¥32,000ã€â†’ 32000
   - ä¾¡æ ¼ãŒãªã„å ´åˆã¯ offers ã‚’ç©ºé…åˆ— [] ã«ã™ã‚‹

4. å‡ºåŠ›å½¢å¼: å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ã€å‡ºåŠ›ä¾‹ã€‘
{{
  "productName": "Y-27632 dihydrochloride",
  "modelNumber": "146986-50-7",
  "manufacturer": "Sigma-Aldrich",
  "offers": [
    {{"size": "1mg", "price": 34000, "inStock": true}},
    {{"size": "5mg", "price": 54000, "inStock": true}}
  ]
}}

ã€HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
{html_content}

ã€ã‚½ãƒ¼ã‚¹URLã€‘
{url}

å¿…ãšJSONå½¢å¼ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        logger.log(f"  ğŸ“¨ Gemini APIå¿œç­”å—ä¿¡ ({len(response_text)} chars)", "DEBUG")
        
        # JSONã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        response_text = re.sub(r'^```json\s*', '', response_text)
        response_text = re.sub(r'^```\s*', '', response_text)
        response_text = re.sub(r'\s*```$', '', response_text)
        response_text = response_text.strip()
        
        # JSONãƒ‘ãƒ¼ã‚¹
        product_info = json.loads(response_text)
        
        # ãƒ‡ãƒ¼ã‚¿å‹æ¤œè¨¼
        if 'offers' in product_info and isinstance(product_info['offers'], list):
            valid_offers = []
            for offer in product_info['offers']:
                if 'price' in offer:
                    try:
                        if isinstance(offer['price'], str):
                            price_str = offer['price'].replace(',', '').replace('Â¥', '').replace('å††', '').replace('$', '').replace('â‚¬', '').strip()
                            offer['price'] = float(price_str)
                        else:
                            offer['price'] = float(offer['price'])
                        
                        if offer['price'] > 0:
                            valid_offers.append(offer)
                    except:
                        pass
            
            product_info['offers'] = valid_offers
        
        if product_info.get('offers'):
            logger.log(f"  âœ… {len(product_info['offers'])}ä»¶ã®ä¾¡æ ¼æƒ…å ±ã‚’æŠ½å‡º", "INFO")
            for i, offer in enumerate(product_info['offers'][:3]):
                logger.log(f"    - {offer.get('size', 'N/A')}: Â¥{int(offer.get('price', 0)):,}", "DEBUG")
        else:
            logger.log(f"  âš ï¸ ä¾¡æ ¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
        
        return product_info
        
    except json.JSONDecodeError as e:
        logger.log(f"  âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None
    except Exception as e:
        logger.log(f"  âŒ è£½å“æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None

def main():
    st.markdown('<h1 class="main-header">ğŸ§ª åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆBrowser APIç‰ˆï¼‰</h1>', unsafe_allow_html=True)
    
    if BROWSER_API_CONFIG['available']:
        st.markdown(
            '<div class="api-status api-success">âœ… Browser APIæ¥ç¶š: BRIGHT DATA (Zone: scraping_browser1)</div>',
            unsafe_allow_html=True
        )
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        product_name = st.text_input(
            "ğŸ” è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value="Y-27632",
            placeholder="ä¾‹: Y-27632, DMSO, Trizol, Quinpirole"
        )
    
    with col2:
        max_sites = st.number_input(
            "æœ€å¤§æ¤œç´¢ã‚µã‚¤ãƒˆæ•°",
            min_value=1,
            max_value=11,
            value=11,
            step=1
        )
    
    st.markdown("---")
    
    if st.button("ğŸš€ æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True):
        if not product_name:
            st.warning("âš ï¸ è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        st.markdown("### ğŸ“ å‡¦ç†ãƒ­ã‚°")
        log_container = st.empty()
        logger = RealTimeLogger(log_container)
        
        start_time = time.time()
        logger.log(f"ğŸš€ å‡¦ç†é–‹å§‹: {product_name}", "INFO")
        logger.log(f"ğŸŒ Browser API: BRIGHT DATA (Zone: scraping_browser1)", "INFO")
        logger.log(f"ğŸ¯ å¯¾è±¡ã‚µã‚¤ãƒˆæ•°: {max_sites}ã‚µã‚¤ãƒˆ", "INFO")
        
        model = setup_gemini()
        if not model:
            st.error("âŒ Gemini APIã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        all_products = []
        sites_to_search = dict(list(TARGET_SITES.items())[:max_sites])
        
        for site_idx, (site_key, site_info) in enumerate(sites_to_search.items(), 1):
            logger.log(f"\n--- ã‚µã‚¤ãƒˆ {site_idx}/{max_sites} ---", "INFO")
            
            search_results = search_with_strategy(product_name, site_info, logger)
            
            if not search_results:
                logger.log(f"â­ï¸  æ¬¡ã®ã‚µã‚¤ãƒˆã¸", "DEBUG")
                time.sleep(2)
                continue
            
            # æœ€ã‚‚ã‚¹ã‚³ã‚¢ãŒé«˜ã„URLã‚’ä½¿ç”¨
            search_results.sort(key=lambda x: x.get('score', 0), reverse=True)
            result = search_results[0]
            
            logger.log(f"ğŸ¯ ãƒˆãƒƒãƒ—URL: {result['url'][:80]}...", "INFO")
            
            # Browser APIçµŒç”±ã§ãƒšãƒ¼ã‚¸å–å¾—
            html_content = fetch_page_with_browser(result['url'], logger)
            
            if html_content:
                page_info = extract_product_info_from_page(html_content, product_name, result['url'], model, logger)
                
                if page_info:
                    page_info['source_site'] = result['site']
                    page_info['source_url'] = result['url']
                    all_products.append(page_info)
                    logger.log(f"âœ… {result['site']}: è£½å“æƒ…å ±å–å¾—æˆåŠŸ", "INFO")
                else:
                    logger.log(f"âš ï¸ {result['site']}: AIè§£æå¤±æ•—", "WARNING")
            else:
                logger.log(f"âŒ {result['site']}: ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—", "ERROR")
            
            time.sleep(2)
        
        elapsed_time = time.time() - start_time
        logger.log(f"\nğŸ‰ å‡¦ç†å®Œäº†: {elapsed_time:.1f}ç§’", "INFO")
        logger.log(f"ğŸ“Š å–å¾—æˆåŠŸ: {len(all_products)}/{max_sites}ã‚µã‚¤ãƒˆ", "INFO")
        
        st.markdown("---")
        st.markdown("## ğŸ“‹ æ¤œç´¢çµæœ")
        
        if not all_products:
            st.error("âŒ è£½å“æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: è£½å“åã‚’å¤‰æ›´ã™ã‚‹ã‹ã€æ¤œç´¢å¯¾è±¡ã‚µã‚¤ãƒˆã‚’èª¿æ•´ã—ã¦ãã ã•ã„")
            return
        
        with_price = [p for p in all_products if p.get('offers')]
        without_price = [p for p in all_products if not p.get('offers')]
        
        st.success(f"âœ… {len(all_products)}ä»¶ã®è£½å“æƒ…å ±ã‚’å–å¾—ï¼ˆä¾¡æ ¼æƒ…å ±ã‚ã‚Š: {len(with_price)}ä»¶ã€å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’ï¼‰")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¡¨ç¤º
        table_data = []
        for product in all_products:
            base_info = {
                'è£½å“å': product.get('productName', 'N/A'),
                'è²©å£²å…ƒ': product.get('source_site', 'N/A'),
                'å‹ç•ª': product.get('modelNumber', 'N/A'),
                'ãƒ¡ãƒ¼ã‚«ãƒ¼': product.get('manufacturer', 'N/A')
            }
            
            if 'offers' in product and product['offers']:
                for offer in product['offers']:
                    row = base_info.copy()
                    row['å®¹é‡'] = offer.get('size', 'N/A')
                    
                    try:
                        price = offer.get('price', 0)
                        if isinstance(price, (int, float)) and price > 0:
                            row['ä¾¡æ ¼'] = f"Â¥{int(price):,}"
                        else:
                            row['ä¾¡æ ¼'] = 'N/A'
                    except:
                        row['ä¾¡æ ¼'] = 'N/A'
                    
                    row['åœ¨åº«æœ‰ç„¡'] = 'æœ‰' if offer.get('inStock') else 'ç„¡'
                    table_data.append(row)
            else:
                row = base_info.copy()
                row['å®¹é‡'] = 'N/A'
                row['ä¾¡æ ¼'] = 'N/A'
                row['åœ¨åº«æœ‰ç„¡'] = 'N/A'
                table_data.append(row)
        
        if table_data:
            df_display = pd.DataFrame(table_data)
            st.dataframe(df_display, use_container_width=True, height=600)
        
        # CSVå‡ºåŠ›
        st.markdown("---")
        st.markdown("## ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        export_data = []
        for product in all_products:
            base_info = {
                'è£½å“å': product.get('productName', 'N/A'),
                'å‹ç•ª': product.get('modelNumber', 'N/A'),
                'ãƒ¡ãƒ¼ã‚«ãƒ¼': product.get('manufacturer', 'N/A'),
                'è²©å£²å…ƒ': product.get('source_site', 'N/A'),
                'URL': product.get('source_url', 'N/A')
            }
            
            if 'offers' in product and product['offers']:
                for offer in product['offers']:
                    row = base_info.copy()
                    row['ã‚µã‚¤ã‚º'] = offer.get('size', 'N/A')
                    
                    try:
                        price = offer.get('price', 0)
                        row['ä¾¡æ ¼'] = int(price) if isinstance(price, (int, float)) else 0
                    except:
                        row['ä¾¡æ ¼'] = 0
                    
                    row['åœ¨åº«'] = 'æœ‰' if offer.get('inStock') else 'ç„¡'
                    export_data.append(row)
            else:
                row = base_info.copy()
                row['ã‚µã‚¤ã‚º'] = 'N/A'
                row['ä¾¡æ ¼'] = 0
                row['åœ¨åº«'] = 'N/A'
                export_data.append(row)
        
        df = pd.DataFrame(export_data)
        
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        csv_data = csv_buffer.getvalue()
        
        st.download_button(
            label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=f"chemical_prices_{product_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True
        )

if __name__ == "__main__":
    main()

"""
åŒ–å­¦è©¦è–¬æƒ…å ±åé›†ã‚¢ãƒ—ãƒª v3.5
- é«˜é€ŸåŒ–ç‰ˆï¼ˆ550ç§’ â†’ 200-300ç§’ç›®æ¨™ï¼‰
- ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯å¯¾å¿œï¼ˆã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªãƒªãƒ³ã‚¯ï¼‰
- Bright Data Browser API + SERP APIçµ±åˆ
- è£½å“åé¡ä¼¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° + 404/ã‚¨ãƒ©ãƒ¼æ¤œå‡º
- Gemini 2.5 Proä½¿ç”¨
"""

import streamlit as st
import asyncio
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
import requests
import json
import time
from datetime import datetime
import pandas as pd
from io import StringIO
import re
from difflib import SequenceMatcher
import html
import urllib.parse
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# ========== è¨­å®š ==========
BRIGHT_DATA_CONFIG = {
    "browser_api": {
        "host": "brd.superproxy.io",
        "port": 9515,
        "username": "brd-customer-hl_d0ba4768-zone-scraping_browser1",
        "password": "ohwvpqbxcj3q"
    },
    "serp_api": {
        "url": "https://api.brightdata.com/serp/req",
        "username": "brd-customer-hl_d0ba4768-zone-serp_api1",
        "password": "ohwvpqbxcj3q"
    }
}

GEMINI_API_KEY = "AIzaSyAXVsix-5q5_VZdBH00T9EwGmTK7iCAESI"
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent"

EC_SITES = [
    {"name": "ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª", "domain": "cosmobio.co.jp"},
    {"name": "ãƒ•ãƒŠã‚³ã‚·", "domain": "funakoshi.co.jp"},
    {"name": "AXEL", "domain": "axel.as-1.co.jp"},
    {"name": "Selleck", "domain": "selleck.co.jp"},
    {"name": "MCE", "domain": "medchemexpress.com"},
    {"name": "ãƒŠã‚«ãƒ©ã‚¤", "domain": "nacalai.co.jp"},
    {"name": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ å’Œå…‰", "domain": "labchem-wako.fujifilm.com"},
    {"name": "é–¢æ±åŒ–å­¦", "domain": "kanto.co.jp"},
    {"name": "TCI", "domain": "tcichemicals.com"},
    {"name": "Merck", "domain": "sigmaaldrich.com"},
    {"name": "å’Œå…‰ç´”è–¬", "domain": "wako-chem.co.jp"}
]

# ========== é«˜é€ŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆv3.5ï¼‰ ==========
SPEED_CONFIG = {
    "page_timeout": 30000,        # 60ç§’ â†’ 30ç§’
    "wait_time": 2000,            # 5ç§’ â†’ 2ç§’
    "retry_count": 2,             # 3å› â†’ 2å›
    "serp_timeout": 15,           # 20ç§’ â†’ 15ç§’
    "gemini_timeout": 20,         # 30ç§’ â†’ 20ç§’
    "min_html_size": 5000         # 404æ¤œå‡º
}

SIMILARITY_THRESHOLD = 0.5  # è£½å“åé¡ä¼¼åº¦é–¾å€¤

# ========== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ==========

def clean_url(url):
    """URLã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆUnicode/HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼‰"""
    if not url:
        return url
    url = html.unescape(url)
    url = urllib.parse.unquote(url)
    url = url.strip()
    return url

def calculate_similarity(str1, str2):
    """2ã¤ã®æ–‡å­—åˆ—ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆ0.0ï½1.0ï¼‰"""
    str1_clean = str1.lower().strip()
    str2_clean = str2.lower().strip()
    return SequenceMatcher(None, str1_clean, str2_clean).ratio()

def is_likely_404_page(html_content):
    """404ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®å¯èƒ½æ€§ã‚’åˆ¤å®š"""
    if len(html_content) < SPEED_CONFIG["min_html_size"]:
        return True
    
    error_keywords = [
        "404", "not found", "ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
        "ãŠæ¢ã—ã®ãƒšãƒ¼ã‚¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
        "è©²å½“ã™ã‚‹å•†å“ãŒã‚ã‚Šã¾ã›ã‚“"
    ]
    
    html_lower = html_content.lower()
    count = sum(1 for keyword in error_keywords if keyword in html_lower)
    
    return count >= 2

# ========== SERP APIï¼ˆGoogleæ¤œç´¢ï¼‰ ==========

def search_urls_serp_api(query, site_domain, max_results=3):
    """SERP APIã§Googleæ¤œç´¢ã‚’å®Ÿè¡Œ"""
    search_query = f"{query} site:{site_domain}"
    
    payload = [{
        "url": "https://www.google.com/search",
        "q": search_query,
        "gl": "jp",
        "hl": "ja",
        "num": max_results
    }]
    
    try:
        logger.info(f"ğŸ” SERP APIæ¤œç´¢: {site_domain}")
        response = requests.post(
            BRIGHT_DATA_CONFIG["serp_api"]["url"],
            auth=(
                BRIGHT_DATA_CONFIG["serp_api"]["username"],
                BRIGHT_DATA_CONFIG["serp_api"]["password"]
            ),
            headers={"Content-Type": "application/json"},
            json=payload,
            timeout=SPEED_CONFIG["serp_timeout"]
        )
        
        if response.status_code != 200:
            logger.warning(f"âš ï¸ SERP APIã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}")
            return []
        
        results = response.json()
        if not results or len(results) == 0:
            logger.warning(f"âš ï¸ SERP APIçµæœãªã—: {site_domain}")
            return []
        
        result_data = results[0]
        organic_results = result_data.get("organic", [])
        
        urls = []
        for item in organic_results[:max_results]:
            url = item.get("link")
            if url:
                cleaned_url = clean_url(url)
                urls.append(cleaned_url)
        
        logger.info(f"âœ… URLç™ºè¦‹: {len(urls)}ä»¶ ({site_domain})")
        return urls
    
    except requests.Timeout:
        logger.error(f"â±ï¸ SERP APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {site_domain}")
        return []
    except Exception as e:
        logger.error(f"âŒ SERP APIã‚¨ãƒ©ãƒ¼: {str(e)[:100]}")
        return []

# ========== Browser APIï¼ˆãƒšãƒ¼ã‚¸å–å¾—ï¼‰ ==========

async def fetch_page_with_browser_api(url, retries=SPEED_CONFIG["retry_count"]):
    """Browser APIã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
    ws_endpoint = (
        f"wss://{BRIGHT_DATA_CONFIG['browser_api']['username']}:"
        f"{BRIGHT_DATA_CONFIG['browser_api']['password']}@"
        f"{BRIGHT_DATA_CONFIG['browser_api']['host']}:"
        f"{BRIGHT_DATA_CONFIG['browser_api']['port']}"
    )
    
    for attempt in range(retries):
        try:
            async with async_playwright() as p:
                browser = await p.chromium.connect_over_cdp(ws_endpoint)
                context = browser.contexts[0] if browser.contexts else await browser.new_context()
                page = await context.new_page()
                
                logger.info(f"ğŸŒ ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {url[:60]}...")
                
                response = await page.goto(
                    url,
                    wait_until="domcontentloaded",
                    timeout=SPEED_CONFIG["page_timeout"]
                )
                
                if response and response.status in [403, 404, 500]:
                    logger.warning(f"âš ï¸ HTTP {response.status}: {url[:60]}")
                    await browser.close()
                    if attempt < retries - 1:
                        await asyncio.sleep(1)
                        continue
                    return None
                
                await asyncio.sleep(SPEED_CONFIG["wait_time"] / 1000)
                html_content = await page.content()
                await browser.close()
                
                if is_likely_404_page(html_content):
                    logger.warning(f"ğŸš« 404ãƒšãƒ¼ã‚¸æ¤œå‡º: {len(html_content)} chars")
                    return None
                
                logger.info(f"âœ… ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ: {len(html_content)} chars")
                return html_content
        
        except PlaywrightTimeout:
            logger.warning(f"â±ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (è©¦è¡Œ {attempt+1}/{retries})")
            if attempt < retries - 1:
                await asyncio.sleep(1)
                continue
            return None
        except Exception as e:
            logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt+1}/{retries}): {str(e)[:100]}")
            if attempt < retries - 1:
                await asyncio.sleep(1)
                continue
            return None
    
    return None

# ========== Gemini APIï¼ˆæ§‹é€ åŒ–æŠ½å‡ºï¼‰ ==========

def extract_with_gemini(html_content, query_product, source_url):
    """Gemini APIã§è£½å“æƒ…å ±ã‚’æŠ½å‡ºï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
    
    # ä¾¡æ ¼é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
    price_keywords = ["ä¾¡æ ¼", "å††", "Â¥", "ç¨", "price", "JPY", "é€æ–™"]
    html_lower = html_content.lower()
    keyword_count = sum(1 for kw in price_keywords if kw in html_lower)
    
    if keyword_count == 0:
        logger.warning(f"âš ï¸ ä¾¡æ ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœªæ¤œå‡ºï¼ˆ{len(html_content)} charsï¼‰")
    
    # HTMLã‚’25000æ–‡å­—ã«åˆ¶é™ï¼ˆGemini APIã®ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ï¼‰
    html_snippet = html_content[:25000]
    
    prompt = f"""
ã‚ãªãŸã¯åŒ–å­¦è©¦è–¬ã®ECã‚µã‚¤ãƒˆã‹ã‚‰è£½å“æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

ã€é‡è¦ã€‘ä»¥ä¸‹ã®HTMLã‹ã‚‰ã€Œ{query_product}ã€ã«é–¢ã™ã‚‹è£½å“æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„:
```html
{html_snippet}
```

æŠ½å‡ºãƒ«ãƒ¼ãƒ«:
1. è£½å“åãŒã€Œ{query_product}ã€ã¨ä¸€è‡´ã¾ãŸã¯é¡ä¼¼ã™ã‚‹è£½å“ã®ã¿å¯¾è±¡
2. è¤‡æ•°ã®å®¹é‡/ä¾¡æ ¼ãŒã‚ã‚‹å ´åˆã¯å…¨ã¦æŠ½å‡º
3. åœ¨åº«æƒ…å ±ãŒä¸æ˜ãªå ´åˆã¯ã€Œä¸æ˜ã€
4. ãƒ¡ãƒ¼ã‚«ãƒ¼æƒ…å ±ãŒä¸æ˜ãªå ´åˆã¯ã€Œä¸æ˜ã€

å¿…é ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆJSONé…åˆ—ï¼‰:
```json
[
  {{
    "product_name": "è£½å“å",
    "catalog_number": "å‹ç•ªã¾ãŸã¯CASç•ªå·",
    "manufacturer": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
    "link": "{source_url}",
    "capacity": "å®¹é‡ï¼ˆä¾‹: 1mg, 5mgï¼‰",
    "price": "ä¾¡æ ¼ï¼ˆä¾‹: Â¥34,000ï¼‰",
    "stock_status": "åœ¨åº«æœ‰ç„¡ï¼ˆæœ‰/ç„¡/ä¸æ˜ï¼‰"
  }}
]
```

ã€æ³¨æ„ã€‘è£½å“æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºé…åˆ— [] ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
"""
    
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.1,
            "maxOutputTokens": 2048
        }
    }
    
    try:
        logger.info(f"ğŸ¤– Gemini APIå‘¼ã³å‡ºã—ä¸­...")
        response = requests.post(
            f"{GEMINI_API_URL}?key={GEMINI_API_KEY}",
            headers=headers,
            json=payload,
            timeout=SPEED_CONFIG["gemini_timeout"]
        )
        
        if response.status_code != 200:
            logger.error(f"âŒ Gemini APIã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}")
            return []
        
        result = response.json()
        text_response = result["candidates"][0]["content"]["parts"][0]["text"]
        
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', text_response)
        if json_match:
            json_text = json_match.group(1)
        else:
            json_text = text_response
        
        products = json.loads(json_text)
        
        if not isinstance(products, list):
            logger.warning(f"âš ï¸ Geminiå¿œç­”ãŒé…åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            return []
        
        logger.info(f"âœ… GeminiæŠ½å‡ºæˆåŠŸ: {len(products)}ä»¶")
        return products
    
    except requests.Timeout:
        logger.error(f"â±ï¸ Gemini APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        return []
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)[:100]}")
        return []
    except Exception as e:
        logger.error(f"âŒ Gemini APIã‚¨ãƒ©ãƒ¼: {str(e)[:100]}")
        return []

# ========== ãƒ¡ã‚¤ãƒ³å‡¦ç† ==========

async def collect_product_info(query_product, site_name, site_domain):
    """1ã‚µã‚¤ãƒˆã®è£½å“æƒ…å ±åé›†ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“¦ ã‚µã‚¤ãƒˆ: {site_name} ({site_domain})")
    
    # SERP APIã§URLæ¤œç´¢
    urls = search_urls_serp_api(query_product, site_domain, max_results=3)
    
    if not urls:
        logger.warning(f"âš ï¸ URLæœªç™ºè¦‹: {site_name}")
        return []
    
    all_products = []
    filtered_count = 0
    
    for idx, url in enumerate(urls, 1):
        logger.info(f"\n--- URL {idx}/{len(urls)} ---")
        logger.info(f"ğŸ”— {url}")
        
        # Browser APIã§ãƒšãƒ¼ã‚¸å–å¾—
        html_content = await fetch_page_with_browser_api(url)
        
        if not html_content:
            logger.warning(f"âš ï¸ ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—")
            continue
        
        # Gemini APIã§æŠ½å‡º
        products = extract_with_gemini(html_content, query_product, url)
        
        if not products:
            logger.warning(f"âš ï¸ è£½å“æƒ…å ±ãªã—")
            continue
        
        # è£½å“åé¡ä¼¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        for product in products:
            product_name = product.get("product_name", "")
            similarity = calculate_similarity(query_product, product_name)
            
            if similarity < SIMILARITY_THRESHOLD:
                logger.warning(
                    f"ğŸš« é¡ä¼¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é™¤å¤–: "
                    f"{product_name[:30]} (é¡ä¼¼åº¦: {similarity:.2f})"
                )
                filtered_count += 1
                continue
            
            product["site_name"] = site_name
            all_products.append(product)
            logger.info(f"âœ… è£½å“è¿½åŠ : {product_name[:30]} (é¡ä¼¼åº¦: {similarity:.2f})")
    
    if filtered_count > 0:
        logger.info(f"ğŸš« ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é™¤å¤–: {filtered_count}ä»¶")
    
    logger.info(f"ğŸ“Š {site_name} å–å¾—å®Œäº†: {len(all_products)}ä»¶")
    return all_products

async def collect_all_sites(query_product, progress_bar, status_text):
    """å…¨ã‚µã‚¤ãƒˆã‹ã‚‰è£½å“æƒ…å ±åé›†ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
    all_products = []
    total_sites = len(EC_SITES)
    success_count = 0
    total_filtered = 0
    
    start_time = time.time()
    
    for idx, site in enumerate(EC_SITES, 1):
        status_text.text(f"ğŸ” æ¤œç´¢ä¸­: {site['name']} ({idx}/{total_sites})")
        progress_bar.progress(idx / total_sites)
        
        products = await collect_product_info(
            query_product,
            site['name'],
            site['domain']
        )
        
        if products:
            all_products.extend(products)
            success_count += 1
        
        logger.info(f"â­ï¸ æ¬¡ã®ã‚µã‚¤ãƒˆã¸")
    
    elapsed_time = time.time() - start_time
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ‰ å‡¦ç†å®Œäº†: {elapsed_time:.1f}ç§’")
    logger.info(f"ğŸ“Š å–å¾—æˆåŠŸ: {success_count}/{total_sites}ã‚µã‚¤ãƒˆ")
    if total_filtered > 0:
        logger.info(f"ğŸš« ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é™¤å¤–: {total_filtered}ä»¶ï¼ˆé¡ä¼¼åº¦ < {SIMILARITY_THRESHOLD}ï¼‰")
    
    return all_products

# ========== Streamlit UI ==========

def create_hyperlink_df(df):
    """DataFrameã®ãƒªãƒ³ã‚¯å…ˆåˆ—ã‚’ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªHTMLãƒªãƒ³ã‚¯ã«å¤‰æ›"""
    if df.empty or 'ãƒªãƒ³ã‚¯å…ˆ' not in df.columns:
        return df
    
    df_display = df.copy()
    
    # ãƒªãƒ³ã‚¯å…ˆã‚’HTMLãƒªãƒ³ã‚¯ã«å¤‰æ›
    df_display['ãƒªãƒ³ã‚¯å…ˆ'] = df_display['ãƒªãƒ³ã‚¯å…ˆ'].apply(
        lambda x: f'<a href="{x}" target="_blank">ğŸ”— è£½å“ãƒšãƒ¼ã‚¸</a>' if pd.notna(x) else ''
    )
    
    return df_display

def main():
    st.set_page_config(
        page_title="åŒ–å­¦è©¦è–¬æƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ  v3.5",
        page_icon="ğŸ§ª",
        layout="wide"
    )
    
    st.title("ğŸ§ª åŒ–å­¦è©¦è–¬æƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ  v3.5")
    st.markdown("**é«˜é€ŸåŒ–ç‰ˆ** | Browser API + SERP APIçµ±åˆ | Gemini 2.5 Pro")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        st.write(f"**å¯¾è±¡ã‚µã‚¤ãƒˆæ•°**: {len(EC_SITES)}ã‚µã‚¤ãƒˆ")
        st.write(f"**é¡ä¼¼åº¦é–¾å€¤**: {SIMILARITY_THRESHOLD}")
        st.write(f"**ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**: {SPEED_CONFIG['page_timeout']/1000}ç§’")
        st.write(f"**å¾…æ©Ÿæ™‚é–“**: {SPEED_CONFIG['wait_time']/1000}ç§’")
        
        st.markdown("---")
        st.markdown("### ğŸ“‹ å¯¾è±¡ECã‚µã‚¤ãƒˆ")
        for site in EC_SITES:
            st.markdown(f"- {site['name']}")
    
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
    query = st.text_input(
        "ğŸ” æ¤œç´¢ã™ã‚‹è©¦è–¬åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        placeholder="ä¾‹: Y-27632, Paclitaxel, DMSO"
    )
    
    col1, col2 = st.columns([1, 4])
    with col1:
        search_button = st.button("ğŸš€ æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True)
    
    if search_button:
        if not query:
            st.error("âš ï¸ è©¦è–¬åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        st.markdown("---")
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # éåŒæœŸå‡¦ç†å®Ÿè¡Œ
        products = asyncio.run(collect_all_sites(query, progress_bar, status_text))
        
        progress_bar.empty()
        status_text.empty()
        
        if not products:
            st.warning("âš ï¸ è£½å“æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        df = pd.DataFrame(products)
        
        # åˆ—åã‚’æ—¥æœ¬èªã«å¤‰æ›
        column_mapping = {
            "product_name": "è£½å“å",
            "site_name": "è²©å£²å…ƒ",
            "catalog_number": "å‹ç•ª",
            "manufacturer": "ãƒ¡ãƒ¼ã‚«ãƒ¼",
            "link": "ãƒªãƒ³ã‚¯å…ˆ",
            "capacity": "å®¹é‡",
            "price": "ä¾¡æ ¼",
            "stock_status": "åœ¨åº«æœ‰ç„¡"
        }
        df = df.rename(columns=column_mapping)
        
        # åˆ—é †åºã‚’æ•´ç†
        column_order = ["è£½å“å", "è²©å£²å…ƒ", "å‹ç•ª", "ãƒ¡ãƒ¼ã‚«ãƒ¼", "ãƒªãƒ³ã‚¯å…ˆ", "å®¹é‡", "ä¾¡æ ¼", "åœ¨åº«æœ‰ç„¡"]
        df = df[[col for col in column_order if col in df.columns]]
        
        st.success(f"âœ… æ¤œç´¢å®Œäº†: {len(df)}ä»¶ã®è£½å“æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
        
        # HTMLãƒªãƒ³ã‚¯ä»˜ãDataFrameã‚’ä½œæˆ
        df_display = create_hyperlink_df(df)
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºï¼ˆHTMLãƒªãƒ³ã‚¯æœ‰åŠ¹åŒ–ï¼‰
        st.markdown("### ğŸ“Š æ¤œç´¢çµæœ")
        st.write(df_display.to_html(escape=False, index=False), unsafe_allow_html=True)
        
        # CSVå‡ºåŠ›
        st.markdown("---")
        st.markdown("### ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        timestamp = datetime.now().strftime("%Y-%m-%dT%H-%M")
        csv_filename = f"{timestamp}_export.csv"
        
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=True, encoding='utf-8-sig')
        csv_data = csv_buffer.getvalue()
        
        st.download_button(
            label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=csv_filename,
            mime="text/csv"
        )
        
        st.info(f"ğŸ’¡ ãƒ’ãƒ³ãƒˆ: Excelã§é–‹ãå ´åˆã¯ UTF-8 BOM å½¢å¼ã§ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")

if __name__ == "__main__":
    main()

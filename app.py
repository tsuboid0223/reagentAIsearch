# -*- coding: utf-8 -*-
"""
è£½å“èª¿é”AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
ï¼ˆ404å¯¾ç­–ç‰ˆ: URLæ¤œè¨¼ + ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
"""

# ==============================================================================
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ==============================================================================
import streamlit as st
import pandas as pd
import time
import urllib.parse
import requests
from bs4 import BeautifulSoup
import json
import urllib3
import random
import logging
import sys
from datetime import datetime

# è¨ºæ–­ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==============================================================================
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°è¡¨ç¤ºç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
# ==============================================================================
class RealTimeLogger:
    """Streamlitã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        self.log_container = st.empty()
        self.logs = []
    
    def add(self, message: str, level: str = "info"):
        """ãƒ­ã‚°ã‚’è¿½åŠ ã—ã¦å³åº§ã«è¡¨ç¤º"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        
        if level == "info":
            icon = "â„¹ï¸"
        elif level == "success":
            icon = "âœ…"
        elif level == "warning":
            icon = "âš ï¸"
        elif level == "error":
            icon = "âŒ"
        else:
            icon = "ğŸ“"
        
        log_entry = f"{icon} [{timestamp}] {message}"
        self.logs.append(log_entry)
        
        # æœ€æ–°20ä»¶ã®ãƒ­ã‚°ã‚’è¡¨ç¤º
        display_logs = self.logs[-20:]
        self.log_container.text_area(
            "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°",
            "\n".join(display_logs),
            height=300,
            key=f"log_{len(self.logs)}"
        )
        
        # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«ã‚‚å‡ºåŠ›
        logger.info(message)
        sys.stdout.flush()
    
    def clear(self):
        """ãƒ­ã‚°ã‚’ã‚¯ãƒªã‚¢"""
        self.logs = []
        self.log_container.empty()

# ==============================================================================
# URLæ¤œè¨¼é–¢æ•°ï¼ˆæ–°è¦è¿½åŠ ï¼‰
# ==============================================================================

def validate_url_quick(url: str, rt_logger: RealTimeLogger, timeout: int = 5) -> bool:
    """
    URLãŒæœ‰åŠ¹ã‹ã‚’é«˜é€Ÿãƒã‚§ãƒƒã‚¯ï¼ˆHEADãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
    """
    try:
        rt_logger.add(f"  URLæ¤œè¨¼ä¸­: {url[:60]}...", "info")
        
        # HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§è»½é‡ãƒã‚§ãƒƒã‚¯
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.head(url, headers=headers, allow_redirects=True, timeout=timeout)
        
        if response.status_code == 404:
            rt_logger.add(f"  âŒ 404 Not Found - ã‚¹ã‚­ãƒƒãƒ—", "warning")
            return False
        elif response.status_code >= 400:
            rt_logger.add(f"  âš ï¸ ã‚¨ãƒ©ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {response.status_code} - ã‚¹ã‚­ãƒƒãƒ—", "warning")
            return False
        else:
            rt_logger.add(f"  âœ… URLæœ‰åŠ¹ (status: {response.status_code})", "success")
            return True
            
    except requests.exceptions.Timeout:
        rt_logger.add(f"  âš ï¸ æ¤œè¨¼ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - è©¦è¡Œã—ã¦ã¿ã‚‹", "warning")
        return True  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®å ´åˆã¯è©¦è¡Œã™ã‚‹
    except Exception as e:
        rt_logger.add(f"  âš ï¸ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)[:50]} - è©¦è¡Œã—ã¦ã¿ã‚‹", "warning")
        return True

# ==============================================================================
# ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹é–¢æ•°ï¼ˆæ–°è¦è¿½åŠ ï¼‰
# ==============================================================================

def get_page_content_direct(url: str, rt_logger: RealTimeLogger, timeout: int = 10) -> dict:
    """
    ãƒ—ãƒ­ã‚­ã‚·ã‚’ä½¿ã‚ãšç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆæœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
    }
    result = {"url": url, "status_code": None, "content": None, "error": None}
    
    try:
        rt_logger.add(f"  ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹è©¦è¡Œ (timeout: {timeout}ç§’)...", "info")
        start_time = time.time()
        
        response = requests.get(url, headers=headers, timeout=timeout)
        elapsed = time.time() - start_time
        
        rt_logger.add(f"  ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹å¿œç­” ({elapsed:.1f}ç§’) - status: {response.status_code}", "info")
        response.raise_for_status()
        
        if len(response.text) < 500:
            rt_logger.add(f"  âš ï¸ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒçŸ­ã„ ({len(response.text)}æ–‡å­—)", "warning")
            result["error"] = "Response too short"
            return result
        
        rt_logger.add(f"  ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸ - {len(response.text)}æ–‡å­—", "success")
        
        soup = BeautifulSoup(response.text, 'html.parser')
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe']):
            tag.decompose()
        
        body_text = soup.body.get_text(separator=' ', strip=True) if soup.body else soup.get_text(separator=' ', strip=True)
        result["content"] = body_text[:18000]
        result["status_code"] = response.status_code
        
        return result
        
    except Exception as e:
        rt_logger.add(f"  ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {str(e)[:50]}", "error")
        result["error"] = str(e)
        return result

# ==============================================================================
# === Bright Data API é€£æºé–¢æ•°ï¼ˆæ”¹å–„ç‰ˆï¼‰===
# ==============================================================================

def get_page_content_with_brightdata(url: str, brd_username: str, brd_password: str, rt_logger: RealTimeLogger, timeout: int = 10) -> dict:
    """
    Scraping Browserã§ç”Ÿbodyãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆ404å¯¾ç­–ç‰ˆï¼‰
    """
    # ã‚¹ãƒ†ãƒƒãƒ—0: URLæ¤œè¨¼
    if not validate_url_quick(url, rt_logger):
        return {"url": url, "status_code": 404, "content": None, "error": "URL validation failed (404)"}
    
    BRD_HOST = 'brd.superproxy.io'
    BRD_PORT = 24000
    proxy_url = f'http://{brd_username}:{brd_password}@{BRD_HOST}:{BRD_PORT}'
    proxies = {'http': proxy_url, 'https': proxy_url}
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'
    }
    result = {"url": url, "status_code": None, "content": None, "error": None}
    
    rt_logger.add(f"Bright Dataæ¥ç¶šé–‹å§‹: {url[:60]}...", "info")
    
    # 1. Scraping Browserè©¦è¡Œ (POST) - ã‚·ãƒ³ãƒ—ãƒ«åŒ–
    payload = {
        'url': url,
        'renderJS': False,  # True â†’ False (é«˜é€ŸåŒ–)
    }
    
    try:
        rt_logger.add(f"  POSTæ¥ç¶šä¸­ (timeout: {timeout}ç§’)...", "info")
        start_time = time.time()
        
        response = requests.post(
            proxy_url,
            json=payload,
            headers=headers,
            proxies=proxies,
            verify=False,
            timeout=timeout
        )
        
        elapsed = time.time() - start_time
        rt_logger.add(f"  POSTå¿œç­”å—ä¿¡ ({elapsed:.1f}ç§’) - status: {response.status_code}", "info")
        
        response.raise_for_status()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ¤œè¨¼
        response_len = len(response.text)
        rt_logger.add(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹é•·: {response_len}æ–‡å­—", "info")
        
        if response_len < 500:
            rt_logger.add(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒçŸ­ã™ãã‚‹ - GETã¸", "warning")
            raise ValueError("Response too short")
        
        # JSON or HTMLåˆ¤å®š
        try:
            data = response.json()
            html = data.get('content', response.text)
            rt_logger.add(f"  JSONå½¢å¼ã§å—ä¿¡", "success")
        except json.JSONDecodeError:
            html = response.text
            rt_logger.add(f"  HTMLå½¢å¼ã§å—ä¿¡", "success")
        
    except requests.exceptions.Timeout:
        rt_logger.add(f"  POSTã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - GETã¸", "error")
        
        # 2. GETãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        full_url = f'{proxy_url}/{url}'
        try:
            rt_logger.add(f"  GETæ¥ç¶šä¸­ (timeout: {timeout//2}ç§’)...", "info")
            start_time = time.time()
            
            response = requests.get(full_url, headers=headers, proxies=proxies, verify=False, timeout=timeout // 2)
            elapsed = time.time() - start_time
            rt_logger.add(f"  GETå¿œç­”å—ä¿¡ ({elapsed:.1f}ç§’) - status: {response.status_code}", "info")
            
            response.raise_for_status()
            
            if len(response.text) < 500:
                rt_logger.add(f"  GETã‚‚çŸ­ã„ - ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã¸", "warning")
                return get_page_content_direct(url, rt_logger)
            
            html = response.text
            rt_logger.add(f"  GETæˆåŠŸ", "success")
            
        except Exception as e2:
            rt_logger.add(f"  GETå¤±æ•— - ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã¸", "error")
            return get_page_content_direct(url, rt_logger)
            
    except Exception as e:
        rt_logger.add(f"  POSTå¤±æ•—: {str(e)[:60]} - ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã¸", "error")
        return get_page_content_direct(url, rt_logger)
    
    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    rt_logger.add(f"  HTMLè§£æä¸­...", "info")
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe']):
            tag.decompose()
        
        body_text = soup.body.get_text(separator=' ', strip=True) if soup.body else soup.get_text(separator=' ', strip=True)
        result["content"] = body_text[:18000]
        result["status_code"] = 200
        
        rt_logger.add(f"  æŠ½å‡ºå®Œäº†: {len(result['content'])}æ–‡å­—", "success")
        
    except Exception as e:
        rt_logger.add(f"  è§£æå¤±æ•—: {str(e)}", "error")
        result["error"] = str(e)
    
    return result


def search_product_urls_with_brightdata(query: str, api_key: str, rt_logger: RealTimeLogger) -> list:
    """Bright Dataã®SERP APIã§Googleæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€URLãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹"""
    rt_logger.add(f"SERP APIæ¤œç´¢é–‹å§‹: {query}", "info")
    
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'
    }
    google_search_url = f"https://www.google.co.jp/search?q={urllib.parse.quote(query)}&hl=ja&gl=jp&ceid=JP:ja"
    payload = {
        'zone': 'serp_api1',
        'url': google_search_url,
        'render': 'js'
    }
    
    wait_time = random.uniform(1, 2)
    rt_logger.add(f"  å¾…æ©Ÿä¸­ ({wait_time:.1f}ç§’)...", "info")
    time.sleep(wait_time)
    
    try:
        rt_logger.add(f"  SERP APIãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...", "info")
        start_time = time.time()
        
        initial_response = requests.post(
            'https://api.brightdata.com/serp/req',
            headers=headers,
            json=payload,
            timeout=30
        )
        
        elapsed = time.time() - start_time
        rt_logger.add(f"  åˆæœŸå¿œç­”å—ä¿¡ ({elapsed:.1f}ç§’) - status: {initial_response.status_code}", "info")
        
        initial_response.raise_for_status()
        response_id = initial_response.headers.get('x-response-id')
        
        if not response_id:
            rt_logger.add(f"  response_idãªã— - ã‚¹ã‚­ãƒƒãƒ—", "warning")
            return []
        
        rt_logger.add(f"  response_idå–å¾—: {response_id[:20]}...", "success")
        result_url = f'https://api.brightdata.com/serp/get_result?response_id={response_id}'
        
        for attempt in range(12):  # 15 â†’ 12ã«çŸ­ç¸®
            wait_time = random.uniform(2, 4)
            rt_logger.add(f"  çµæœå–å¾—å¾…æ©Ÿ (è©¦è¡Œ{attempt+1}/12) - {wait_time:.1f}ç§’...", "info")
            time.sleep(wait_time)
            
            try:
                result_response = requests.get(result_url, headers=headers, timeout=30)
                rt_logger.add(f"  çµæœå¿œç­” - status: {result_response.status_code}", "info")
                
                if result_response.status_code == 200:
                    if not result_response.text:
                        rt_logger.add(f"  ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹", "warning")
                        return []
                    
                    rt_logger.add(f"  HTMLè§£æä¸­... (é•·ã•: {len(result_response.text)}æ–‡å­—)", "info")
                    soup = BeautifulSoup(result_response.text, 'html.parser')
                    result_divs = soup.find_all('div', {'data-ved': True}) or soup.find_all('div', class_='g')
                    
                    rt_logger.add(f"  æ¤œç´¢çµæœdivæ•°: {len(result_divs)}", "info")
                    
                    urls = []
                    for idx, div in enumerate(result_divs):
                        a_tag = div.find('a', href=True)
                        if a_tag and a_tag.get('href') and a_tag.get('href').startswith('http') and not a_tag.get('href').startswith('https://www.google.'):
                            urls.append(a_tag.get('href'))
                            if idx < 3:
                                rt_logger.add(f"    URLç™ºè¦‹: {a_tag.get('href')[:60]}...", "info")
                    
                    unique_urls = list(dict.fromkeys(urls))[:10]
                    rt_logger.add(f"  {len(unique_urls)}ä»¶ã®URLæŠ½å‡ºå®Œäº†", "success")
                    return unique_urls
                    
                elif result_response.status_code == 202:
                    rt_logger.add(f"  ã¾ã å‡¦ç†ä¸­ (202) - å†è©¦è¡Œ", "info")
                else:
                    rt_logger.add(f"  äºˆæœŸã—ãªã„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result_response.status_code}", "warning")
                    return []
                    
            except requests.exceptions.RequestException as e:
                rt_logger.add(f"  çµæœå–å¾—ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ{attempt+1}): {str(e)[:50]}", "error")
                return []
        
        rt_logger.add(f"  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (12å›è©¦è¡Œ)", "warning")
        return []
        
    except requests.exceptions.RequestException as e:
        rt_logger.add(f"  SERP APIã‚¨ãƒ©ãƒ¼: {str(e)[:60]}", "error")
        return []

# ==============================================================================
# === AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–¢é€£é–¢æ•° ===
# ==============================================================================

def analyze_page_and_extract_info(page_content_result: dict, product_name: str, gemini_api_key: str, rt_logger: RealTimeLogger, retry_count: int = 2) -> dict | None:
    """HTMLã‚’Gemini APIã«æ¸¡ã—ã€è£½å“æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹"""
    body_text = page_content_result.get("content")
    if page_content_result.get("error") or not body_text:
        rt_logger.add(f"  ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã— - AIè§£æã‚¹ã‚­ãƒƒãƒ—", "warning")
        return None

    prompt = f"""
ã‚ãªãŸã¯åŒ–å­¦è©¦è–¬ECã‚µã‚¤ãƒˆã®æƒ…å ±æŠ½å‡ºã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚

ã€æŠ½å‡ºå¯¾è±¡è£½å“ã€‘
- è£½å“å: {product_name}

ã€Webãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆã€‘
{body_text}

ã€æŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
1. productName: ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯h1è¦ç´ ã®è£½å“å
2. modelNumber: å‹ç•ª/è£½å“ã‚³ãƒ¼ãƒ‰/ã‚«ã‚¿ãƒ­ã‚°ç•ªå·ï¼ˆä¾‹: ALX-270-333-M001ï¼‰
3. manufacturer: ãƒ¡ãƒ¼ã‚«ãƒ¼/ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼åï¼ˆä¾‹: ENZ, Selleckï¼‰
4. offers: ä¾¡æ ¼è¡¨ã‹ã‚‰ä»¥ä¸‹ã‚’æŠ½å‡º
   - size: å®¹é‡/è¦æ ¼ï¼ˆä¾‹: "1 mg", "5 mg", "1 MG"ï¼‰
   - price: ä¾¡æ ¼ã®æ•°å€¤ã®ã¿ï¼ˆä¾‹: Â¥34,000 â†’ 34000ï¼‰
   - inStock: åœ¨åº«çŠ¶æ³ï¼ˆã€Œåœ¨åº«ã‚ã‚Šã€ã€Œã‚«ãƒ¼ãƒˆã«å…¥ã‚Œã‚‹ã€ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°trueã€ãã‚Œä»¥å¤–ã¯falseï¼‰

ã€æ³¨æ„äº‹é …ã€‘
- æ–‡å­—åŒ–ã‘ã€Œï¿½ï¿½ã€ã¯ã€ŒÂ¥ã€ã¨ã—ã¦å‡¦ç†ã—ã¦ãã ã•ã„
- ä¾¡æ ¼è¡¨ãŒè¤‡æ•°è¡Œã‚ã‚‹å ´åˆã¯å…¨ã¦æŠ½å‡ºã—ã¦ãã ã•ã„
- æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„é …ç›®ã¯nullã‚’è¿”ã—ã¦ãã ã•ã„
- offersé…åˆ—ã¯å¿…ãšä½œæˆã—ã€ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ç©ºé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„

ã€å‡ºåŠ›å½¢å¼ã€‘
{{
  "productName": "string or null",
  "modelNumber": "string or null", 
  "manufacturer": "string or null",
  "offers": [
    {{"size": "string", "price": number, "inStock": boolean}}
  ]
}}
"""
    
    for attempt in range(retry_count):
        try:
            rt_logger.add(f"  Gemini APIå‘¼ã³å‡ºã— (è©¦è¡Œ{attempt+1}/{retry_count})...", "info")
            start_time = time.time()
            
            payload = {
                "contents": [{"role": "user", "parts": [{"text": prompt}]}],
                "generationConfig": {"responseMimeType": "application/json"}
            }
            api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={gemini_api_key}"
            
            response = requests.post(
                api_url,
                headers={'Content-Type': 'application/json'},
                json=payload,
                timeout=45
            )
            
            elapsed = time.time() - start_time
            rt_logger.add(f"  Geminiå¿œç­”å—ä¿¡ ({elapsed:.1f}ç§’) - status: {response.status_code}", "info")
            
            response.raise_for_status()
            result = response.json()
            
            if not result.get('candidates'):
                rt_logger.add(f"  candidates ãªã— - ãƒªãƒˆãƒ©ã‚¤", "warning")
                if attempt < retry_count - 1:
                    time.sleep(2)
                    continue
                return None
            
            response_text = result['candidates'][0]['content']['parts'][0]['text']
            rt_logger.add(f"  JSONè§£æä¸­... (é•·ã•: {len(response_text)}æ–‡å­—)", "info")
            
            raw_data = json.loads(response_text)
            
            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if raw_data and isinstance(raw_data, dict):
                offers_count = len(raw_data.get("offers", []))
                rt_logger.add(f"  æŠ½å‡ºæˆåŠŸ: offers {offers_count}ä»¶", "success")
                
                if offers_count > 0:
                    return raw_data
                elif attempt < retry_count - 1:
                    rt_logger.add(f"  offers ãªã— - ãƒªãƒˆãƒ©ã‚¤", "warning")
                    time.sleep(2)
                    continue
            
            return raw_data if isinstance(raw_data, dict) else None
            
        except json.JSONDecodeError as e:
            rt_logger.add(f"  JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)[:50]}", "error")
            if attempt < retry_count - 1:
                time.sleep(2)
            else:
                return None
                
        except requests.exceptions.RequestException as e:
            rt_logger.add(f"  APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}", "error")
            if attempt < retry_count - 1:
                time.sleep(2)
            else:
                return None
    
    return None

# ==============================================================================
# === çµ±æ‹¬ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ===
# ==============================================================================

def orchestrator_agent(product_info: dict, gemini_api_key: str, brightdata_api_key: str, brd_username: str, brd_password: str, preferred_sites: list, debug_mode: bool = False) -> tuple[list, list]:
    """ä¸€é€£ã®å‡¦ç†ã‚’çµ±æ‹¬ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆ404å¯¾ç­–ç‰ˆï¼‰"""
    product_name = product_info['ProductName']
    manufacturer = product_info.get('Manufacturer', '')
    
    st.subheader(f"ã€çµ±æ‹¬ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‘ \"{product_name}\" ã®æƒ…å ±åé›†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°åˆæœŸåŒ–
    rt_logger = RealTimeLogger()
    rt_logger.add(f"=== çµ±æ‹¬ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–‹å§‹ ===", "success")
    rt_logger.add(f"è£½å“å: {product_name}", "info")
    rt_logger.add(f"ãƒ¡ãƒ¼ã‚«ãƒ¼: {manufacturer}", "info")

    base_query = f"{manufacturer} {product_name}"
    site_map = {
        'ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª': 'cosmobio.co.jp',
        'ãƒ•ãƒŠã‚³ã‚·': 'funakoshi.co.jp',
        'AXEL': 'axel.as-1.co.jp',
        'Selleck': 'selleck.co.jp',
        'MCE': 'medchemexpress.com',
        'Nakarai': 'nacalai.co.jp',
        'FUJIFILM': 'labchem-wako.fujifilm.com',
        'é–¢æ±åŒ–å­¦': 'kanto.co.jp',
        'TCI': 'tcichemicals.com',
        'Merck': 'merck.com',
        'å’Œå…‰ç´”è–¬': 'hpc-j.co.jp'
    }
    search_queries = [f"site:{site_map[site_name]} {base_query}" for site_name in preferred_sites if site_name in site_map]
    search_queries.append(base_query)

    # é€²æ—ãƒãƒ¼åˆæœŸåŒ–
    progress_bar = st.progress(0)
    status_text = st.empty()

    # ã‚¹ãƒ†ãƒƒãƒ—1: URLæŠ½å‡º (0-20%)
    status_text.text("â³ URLæŠ½å‡ºä¸­...")
    progress_bar.progress(0.05)
    rt_logger.add(f"--- ã‚¹ãƒ†ãƒƒãƒ—1: URLæŠ½å‡º ({len(search_queries)}ä»¶ã®ã‚¯ã‚¨ãƒª) ---", "success")
    
    all_urls = []
    num_queries = len(search_queries)
    
    for i, query in enumerate(search_queries):
        rt_logger.add(f"ã‚¯ã‚¨ãƒª {i+1}/{num_queries}: {query}", "info")
        urls = search_product_urls_with_brightdata(query, brightdata_api_key, rt_logger)
        all_urls.extend(urls)
        
        progress = 0.05 + (i / num_queries) * 0.15
        progress_bar.progress(progress)
        status_text.text(f"â³ URLæŠ½å‡ºä¸­... ({i+1}/{num_queries})")
    
    unique_urls = list(dict.fromkeys(all_urls))[:15]
    
    rt_logger.add(f"=== URLæŠ½å‡ºå®Œäº†: {len(unique_urls)}ä»¶ ===", "success")
    
    if not unique_urls:
        st.error("âŒ æ¤œç´¢çµæœã‹ã‚‰URLã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return [], []
    
    # URLä¸€è¦§è¡¨ç¤º
    with st.expander(f"ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸURLä¸€è¦§ ({len(unique_urls)}ä»¶)"):
        for idx, url in enumerate(unique_urls):
            st.text(f"{idx+1}. {url}")
    
    progress_bar.progress(0.2)
    status_text.text("âœ… URLæŠ½å‡ºå®Œäº† (20%)")

    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒšãƒ¼ã‚¸å–å¾— (20-70%)
    status_text.text("â³ Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
    rt_logger.add(f"--- ã‚¹ãƒ†ãƒƒãƒ—2: ãƒšãƒ¼ã‚¸å–å¾— ({len(unique_urls)}ä»¶) ---", "success")
    
    all_page_content_results = []
    success_count = 0
    fail_count = 0

    for i, url in enumerate(unique_urls):
        status_text.text(f"â³ ãƒšãƒ¼ã‚¸å–å¾—ä¸­ ({i + 1}/{len(unique_urls)})")
        rt_logger.add(f"========== URL {i+1}/{len(unique_urls)} ==========", "info")
        rt_logger.add(f"{url}", "info")
        
        try:
            page_result = get_page_content_with_brightdata(url, brd_username, brd_password, rt_logger, timeout=10)
            all_page_content_results.append(page_result)
            
            if page_result.get('content') and len(page_result.get('content', '')) > 1000:
                success_count += 1
                rt_logger.add(f"âœ… æˆåŠŸã‚«ã‚¦ãƒ³ãƒˆ: {success_count}", "success")
            else:
                fail_count += 1
                rt_logger.add(f"âš ï¸ å¤±æ•—ã‚«ã‚¦ãƒ³ãƒˆ: {fail_count}", "warning")
        
        except Exception as e:
            fail_count += 1
            rt_logger.add(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}", "error")
            all_page_content_results.append({"url": url, "error": str(e)})
        
        # é€²æ—æ›´æ–°
        progress = 0.2 + (i + 1) / len(unique_urls) * 0.5
        progress_bar.progress(progress)
        
        # æ—©æœŸçµ‚äº†åˆ¤å®š
        if success_count >= 5 and i >= 8:
            rt_logger.add(f"æ—©æœŸçµ‚äº†: {success_count}ä»¶æˆåŠŸ", "success")
            break

    rt_logger.add(f"=== ãƒšãƒ¼ã‚¸å–å¾—å®Œäº†: æˆåŠŸ {success_count}ä»¶ / å¤±æ•— {fail_count}ä»¶ ===", "success")
    progress_bar.progress(0.7)
    status_text.text("âœ… ãƒšãƒ¼ã‚¸å–å¾—å®Œäº† (70%)")

    # ã‚¹ãƒ†ãƒƒãƒ—3: AIè§£æ (70-100%)
    status_text.text("â³ AIã§ãƒšãƒ¼ã‚¸ã‚’åˆ†æä¸­...")
    rt_logger.add(f"--- ã‚¹ãƒ†ãƒƒãƒ—3: AIè§£æ ---", "success")
    
    found_pages_data = []
    successful_contents = [
        res for res in all_page_content_results 
        if res.get("content") and len(res.get("content", "")) > 1000 and not res.get("error")
    ]
    
    rt_logger.add(f"è§£æå¯¾è±¡: {len(successful_contents)}ä»¶", "info")
    
    if not successful_contents:
        st.warning("âš ï¸ æœ‰åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        progress_bar.progress(1.0)
        status_text.text("âš ï¸ å®Œäº† (100%) - ãƒ‡ãƒ¼ã‚¿ãªã—")
        return [], all_page_content_results
    
    for i, content_res in enumerate(successful_contents):
        status_text.text(f"â³ AIåˆ†æä¸­... ({i + 1}/{len(successful_contents)})")
        rt_logger.add(f"========== AIè§£æ {i+1}/{len(successful_contents)} ==========", "info")
        rt_logger.add(f"{content_res.get('url')}", "info")
        
        try:
            page_details = analyze_page_and_extract_info(content_res, product_name, gemini_api_key, rt_logger)
            
            if page_details and page_details.get("offers"):
                page_details['sourceUrl'] = content_res.get("url")
                found_pages_data.append(page_details)
                rt_logger.add(f"âœ… è£½å“æƒ…å ±è¿½åŠ : {len(found_pages_data)}ä»¶ç›®", "success")
        
        except Exception as e:
            rt_logger.add(f"âŒ AIè§£æã‚¨ãƒ©ãƒ¼: {str(e)}", "error")
        
        progress = 0.7 + (i + 1) / len(successful_contents) * 0.3
        progress_bar.progress(progress)

    progress_bar.progress(1.0)
    status_text.text("âœ… å®Œäº† (100%)")
    rt_logger.add(f"=== çµ±æ‹¬ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Œäº†: {len(found_pages_data)}ãƒšãƒ¼ã‚¸ã‹ã‚‰è£½å“æƒ…å ±ã‚’æŠ½å‡º ===", "success")
    
    return found_pages_data, all_page_content_results

# ==============================================================================
# === Streamlit UI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³éƒ¨åˆ† ===
# ==============================================================================

st.set_page_config(layout="wide")
st.title("è£½å“èª¿é”AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ğŸ”¬")

st.sidebar.header("âš™ï¸ APIã‚­ãƒ¼è¨­å®š")
try:
    gemini_api_key = st.secrets["GOOGLE_API_KEY"]
    brightdata_api_key = st.secrets["BRIGHTDATA_API_KEY"]
    brightdata_username = st.secrets["BRIGHTDATA_USERNAME"]
    brightdata_password = st.secrets["BRIGHTDATA_PASSWORD"]
    st.sidebar.success("âœ… APIã‚­ãƒ¼ã¨èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚")
except KeyError as e:
    st.sidebar.error("âŒ Streamlit Secretsã«å¿…è¦ãªæƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    logger.error(f"Missing secret key: {str(e)}")
    gemini_api_key, brightdata_api_key, brightdata_username, brightdata_password = "", "", "", ""

st.sidebar.header("ğŸ” æ¤œç´¢æ¡ä»¶")
product_name_input = st.sidebar.text_input("è£½å“å (å¿…é ˆ)", placeholder="ä¾‹: Y27632")
manufacturer_input = st.sidebar.text_input("ãƒ¡ãƒ¼ã‚«ãƒ¼", placeholder="ä¾‹: Selleck")
min_price_input = st.sidebar.number_input("æœ€ä½ä¾¡æ ¼ (å††)", min_value=0, value=0, step=100)
max_price_input = st.sidebar.number_input("æœ€é«˜ä¾¡æ ¼ (å††)", min_value=0, value=0, step=100)
debug_mode_checkbox = st.sidebar.checkbox("ğŸ”§ ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
search_button = st.sidebar.button("ğŸš€ æ¤œç´¢é–‹å§‹", type="primary")

if search_button:
    if not all([gemini_api_key, brightdata_api_key, brightdata_username, brightdata_password]):
        st.error("âŒ APIã‚­ãƒ¼ã¾ãŸã¯èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    elif not product_name_input:
        st.error("âŒ è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        logger.info(f"Search started - Product: {product_name_input}, Manufacturer: {manufacturer_input}")
        
        product_info = {
            'ProductName': product_name_input,
            'Manufacturer': manufacturer_input
        }
        preferred_sites = [
            'ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª', 'ãƒ•ãƒŠã‚³ã‚·', 'AXEL', 'Selleck', 'MCE',
            'Nakarai', 'FUJIFILM', 'é–¢æ±åŒ–å­¦', 'TCI', 'Merck', 'å’Œå…‰ç´”è–¬'
        ]
        
        pages_list, log_data = orchestrator_agent(
            product_info,
            gemini_api_key,
            brightdata_api_key,
            brightdata_username,
            brightdata_password,
            preferred_sites,
            debug_mode=debug_mode_checkbox
        )
        
        final_results = []
        input_date = pd.Timestamp.now().strftime('%Y-%m-%d')
        
        if pages_list:
            for page_data in pages_list:
                for offer_item in page_data.get('offers', []):
                    try:
                        price = int(float(offer_item.get('price', 0)))
                    except (ValueError, TypeError):
                        price = 0
                    
                    final_results.append({
                        'å…¥åŠ›æ—¥': input_date,
                        'è£½å“å': page_data.get('productName', 'N/A'),
                        'å‹ç•ª/è£½å“ç•ªå·': page_data.get('modelNumber', 'N/A'),
                        'ä»•æ§˜': offer_item.get('size', 'N/A'),
                        'ãƒ¡ãƒ¼ã‚«ãƒ¼': page_data.get('manufacturer', 'N/A'),
                        'ãƒªã‚¹ãƒˆå˜ä¾¡': price,
                        'åœ¨åº«': 'ã‚ã‚Š' if offer_item.get('inStock') else 'ãªã—/ä¸æ˜',
                        'æƒ…å ±å…ƒURL': page_data.get('sourceUrl', 'N/A')
                    })
        
        if not final_results:
            st.warning("âš ï¸ æ¤œç´¢çµæœã‹ã‚‰æœ‰åŠ¹ãªè£½å“æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            search_term = f"{product_info.get('Manufacturer', '')} {product_info['ProductName']}"
            query_url = f"https://www.google.com/search?q={urllib.parse.quote(search_term)}"
            final_results.append({
                'å…¥åŠ›æ—¥': input_date,
                'è£½å“å': product_info['ProductName'],
                'å‹ç•ª/è£½å“ç•ªå·': 'N/A',
                'ä»•æ§˜': 'N/A',
                'ãƒ¡ãƒ¼ã‚«ãƒ¼': product_info.get('Manufacturer', ''),
                'ãƒªã‚¹ãƒˆå˜ä¾¡': 0,
                'åœ¨åº«': 'ãªã—/ä¸æ˜',
                'æƒ…å ±å…ƒURL': query_url
            })
        
        st.success(f"âœ… æƒ…å ±åé›†å®Œäº† - {len(final_results)}ä»¶ã®çµæœ")

        df_results = pd.DataFrame(final_results)
        
        # ä¾¡æ ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if max_price_input > 0:
            df_results = df_results[df_results['ãƒªã‚¹ãƒˆå˜ä¾¡'] <= max_price_input]
        if min_price_input > 0:
            df_results = df_results[df_results['ãƒªã‚¹ãƒˆå˜ä¾¡'] >= min_price_input]

        st.subheader("ğŸ“Š æ¤œç´¢çµæœ")
        st.dataframe(
            df_results,
            column_config={
                "ãƒªã‚¹ãƒˆå˜ä¾¡": st.column_config.NumberColumn(format="Â¥%d"),
                "æƒ…å ±å…ƒURL": st.column_config.LinkColumn("Link", display_text="é–‹ã")
            },
            use_container_width=True,
            hide_index=True
        )
        
        @st.cache_data
        def convert_df_to_csv(df: pd.DataFrame) -> bytes:
            return df.to_csv(index=False).encode('utf-8-sig')
        
        csv = convert_df_to_csv(df_results)
        st.download_button(
            label="ğŸ“¥ çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv,
            file_name=f"purchase_list_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime='text/csv'
        )

        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
        if debug_mode_checkbox and log_data:
            st.subheader("ğŸ” è©³ç´°ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°")
            
            for idx, log in enumerate(log_data):
                status = log.get('status_code')
                is_error = log.get('error') is not None
                
                if is_error:
                    st.error(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {log['url']}")
                elif status != 200 and status is not None:
                    st.warning(f"âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç•°å¸¸ ({status}): {log['url']}")
                else:
                    st.success(f"âœ… æˆåŠŸ ({status}): {log['url']}")

                with st.expander(f"è©³ç´° - URL {idx+1}"):
                    if is_error:
                        st.write(f"**ã‚¨ãƒ©ãƒ¼:** `{log['error']}`")
                    
                    log_display = log.copy()
                    if log_display.get('content'):
                        content_len = len(log_display['content'])
                        log_display['content'] = (
                            log_display['content'][:1000] + "..."
                            if len(log_display['content']) > 1000
                            else log_display['content']
                        )
                        st.write(f"**Contenté•·:** {content_len}æ–‡å­—")
                    
                    st.json(log_display)

logger.info("Application ready")

import streamlit as st
import requests
import google.generativeai as genai
import time
import re
import json
import pandas as pd
from urllib.parse import quote_plus, urlparse
from io import StringIO
from datetime import datetime

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆæœ¬ç•ªç‰ˆï¼‰",
    page_icon="ğŸ§ª",
    layout="wide"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .site-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        margin: 0.25rem;
        border-radius: 1rem;
        background-color: #e3f2fd;
        color: #1565c0;
        font-size: 0.85rem;
    }
    .success-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        margin: 1rem 0;
    }
    .warning-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #fff3cd;
        border: 1px solid #ffeeba;
        margin: 1rem 0;
    }
    .error-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        margin: 1rem 0;
    }
    .log-container {
        background-color: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 0.5rem;
        padding: 1rem;
        font-family: monospace;
        font-size: 0.85rem;
        max-height: 400px;
        overflow-y: auto;
    }
</style>
""", unsafe_allow_html=True)

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ã‚¯ãƒ©ã‚¹
class RealTimeLogger:
    def __init__(self, container, show_debug=True):
        self.container = container
        self.logs = []
        self.show_debug = show_debug
        
    def log(self, message, level="INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"
        self.logs.append(log_entry)
        
        if self.show_debug:
            with self.container:
                st.code("\n".join(self.logs[-20:]), language="log")  # æœ€æ–°20ä»¶ã‚’è¡¨ç¤º

# Gemini APIè¨­å®š
def setup_gemini():
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        return genai.GenerativeModel('gemini-2.0-flash')
    except Exception as e:
        st.error(f"âŒ Gemini APIè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

# Bright Dataè¨­å®š
def get_brightdata_config():
    try:
        return {
            'api_key': st.secrets["BRIGHTDATA_API_KEY"],
            'username': st.secrets["BRIGHTDATA_USERNAME"],
            'password': st.secrets["BRIGHTDATA_PASSWORD"]
        }
    except Exception as e:
        st.error(f"âŒ Bright Dataè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

# å¯¾è±¡ECã‚µã‚¤ãƒˆã®å®šç¾©ï¼ˆ11ã‚µã‚¤ãƒˆï¼‰
TARGET_SITES = {
    "cosmobio": {"name": "ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª", "domain": "cosmobio.co.jp", "enabled": True},
    "funakoshi": {"name": "ãƒ•ãƒŠã‚³ã‚·", "domain": "funakoshi.co.jp", "enabled": True},
    "axel": {"name": "AXEL", "domain": "axel.as-1.co.jp", "enabled": True},
    "selleck": {"name": "Selleck", "domain": "selleck.co.jp", "enabled": True},
    "mce": {"name": "MCE", "domain": "medchemexpress.com", "enabled": True},
    "nakarai": {"name": "ãƒŠã‚«ãƒ©ã‚¤", "domain": "nacalai.co.jp", "enabled": True},
    "fujifilm": {"name": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ å’Œå…‰", "domain": "labchem-wako.fujifilm.com", "enabled": True},
    "kanto": {"name": "é–¢æ±åŒ–å­¦", "domain": "kanto.co.jp", "enabled": True},
    "tci": {"name": "TCI", "domain": "tcichemicals.com", "enabled": True},
    "merck": {"name": "Merck", "domain": "merck.com", "enabled": True},
    "wako": {"name": "å’Œå…‰ç´”è–¬", "domain": "hpc-j.co.jp", "enabled": True}
}

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯URLï¼ˆY27632ç”¨ï¼‰
FALLBACK_URLS = {
    "Y-27632": {
        "cosmobio": "https://www.cosmobio.co.jp/product/detail/y-27632-dihydrochloride-alx-270-333.asp",
        "funakoshi": "https://www.funakoshi.co.jp/contents/4567",
        "axel": "https://www.axel.as-1.co.jp/asone/d/62-3817-51/",
    }
}

def validate_url(url, logger):
    """URLãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆ404ã‚’é™¤å¤–ï¼‰"""
    try:
        logger.log(f"URLæ¤œè¨¼ä¸­: {url}", "DEBUG")
        response = requests.head(url, timeout=5, allow_redirects=True)
        if response.status_code == 404:
            logger.log(f"404ã‚¨ãƒ©ãƒ¼: {url}", "WARNING")
            return False
        logger.log(f"æœ‰åŠ¹ãªURL: {url} (status: {response.status_code})", "DEBUG")
        return True
    except Exception as e:
        logger.log(f"URLæ¤œè¨¼å¤±æ•—: {url} - {str(e)}", "WARNING")
        return False

def search_product_urls(product_name, sites, logger, max_urls=10):
    """Googleæ¤œç´¢ã§è£½å“URLã‚’å–å¾—ï¼ˆBright DataçµŒç”± + ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    logger.log(f"è£½å“URLæ¤œç´¢é–‹å§‹: {product_name} (æœ€å¤§{max_urls}ä»¶)", "INFO")
    
    all_urls = []
    config = get_brightdata_config()
    
    for site_key, site_info in sites.items():
        if not site_info["enabled"]:
            continue
            
        site_name = site_info["name"]
        domain = site_info["domain"]
        
        logger.log(f"æ¤œç´¢ä¸­: {site_name} ({domain})", "INFO")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯URLã®ç¢ºèª
        if product_name in FALLBACK_URLS and site_key in FALLBACK_URLS[product_name]:
            fallback_url = FALLBACK_URLS[product_name][site_key]
            if validate_url(fallback_url, logger):
                logger.log(f"âœ“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯URLä½¿ç”¨: {fallback_url}", "INFO")
                all_urls.append({
                    'url': fallback_url,
                    'site': site_name,
                    'title': f"{product_name} - {site_name}"
                })
                if len(all_urls) >= max_urls:
                    return all_urls
                continue
        
        # Googleæ¤œç´¢ã‚¯ã‚¨ãƒª
        query = f"{product_name} site:{domain}"
        search_url = f"https://www.google.com/search?q={quote_plus(query)}&num=3"
        
        # Bright DataçµŒç”±ã§è©¦è¡Œï¼ˆPOSTãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        urls_found = False
        if config:
            try:
                logger.log(f"Bright Data POSTçµŒç”±ã§ã‚¢ã‚¯ã‚»ã‚¹è©¦è¡Œ...", "DEBUG")
                proxy_url = "http://brd.superproxy.io:33335"
                proxies = {
                    'http': f"http://{config['username']}:{config['password']}@brd.superproxy.io:33335",
                    'https': f"http://{config['username']}:{config['password']}@brd.superproxy.io:33335"
                }
                
                response = requests.post(
                    search_url,
                    proxies=proxies,
                    timeout=10,
                    headers={'User-Agent': 'Mozilla/5.0'}
                )
                
                if response.status_code == 200:
                    urls = extract_urls_from_html(response.text, domain)
                    if urls:
                        logger.log(f"âœ“ {len(urls)}ä»¶ã®URLã‚’ç™ºè¦‹ï¼ˆBright Data POSTï¼‰", "INFO")
                        for url in urls[:2]:  # ã‚µã‚¤ãƒˆã”ã¨ã«æœ€å¤§2ä»¶
                            if validate_url(url, logger):
                                all_urls.append({
                                    'url': url,
                                    'site': site_name,
                                    'title': f"{product_name} - {site_name}"
                                })
                                if len(all_urls) >= max_urls:
                                    return all_urls
                        urls_found = True
            except Exception as e:
                logger.log(f"Bright Data POSTå¤±æ•—: {str(e)}", "WARNING")
        
        # Bright Data GETãƒ¡ã‚½ãƒƒãƒ‰ã§è©¦è¡Œ
        if not urls_found and config:
            try:
                logger.log(f"Bright Data GETçµŒç”±ã§ã‚¢ã‚¯ã‚»ã‚¹è©¦è¡Œ...", "DEBUG")
                response = requests.get(
                    search_url,
                    proxies=proxies,
                    timeout=10,
                    headers={'User-Agent': 'Mozilla/5.0'}
                )
                
                if response.status_code == 200:
                    urls = extract_urls_from_html(response.text, domain)
                    if urls:
                        logger.log(f"âœ“ {len(urls)}ä»¶ã®URLã‚’ç™ºè¦‹ï¼ˆBright Data GETï¼‰", "INFO")
                        for url in urls[:2]:
                            if validate_url(url, logger):
                                all_urls.append({
                                    'url': url,
                                    'site': site_name,
                                    'title': f"{product_name} - {site_name}"
                                })
                                if len(all_urls) >= max_urls:
                                    return all_urls
                        urls_found = True
            except Exception as e:
                logger.log(f"Bright Data GETå¤±æ•—: {str(e)}", "WARNING")
        
        # ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not urls_found:
            try:
                logger.log(f"ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹è©¦è¡Œ...", "DEBUG")
                response = requests.get(
                    search_url,
                    timeout=10,
                    headers={'User-Agent': 'Mozilla/5.0'}
                )
                
                if response.status_code == 200:
                    urls = extract_urls_from_html(response.text, domain)
                    if urls:
                        logger.log(f"âœ“ {len(urls)}ä»¶ã®URLã‚’ç™ºè¦‹ï¼ˆç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ï¼‰", "INFO")
                        for url in urls[:2]:
                            if validate_url(url, logger):
                                all_urls.append({
                                    'url': url,
                                    'site': site_name,
                                    'title': f"{product_name} - {site_name}"
                                })
                                if len(all_urls) >= max_urls:
                                    return all_urls
            except Exception as e:
                logger.log(f"ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {str(e)}", "WARNING")
        
        time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
    
    logger.log(f"æ¤œç´¢å®Œäº†: {len(all_urls)}ä»¶ã®URLå–å¾—", "INFO")
    return all_urls

def extract_urls_from_html(html_content, domain):
    """HTML ã‹ã‚‰æŒ‡å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã®URLã‚’æŠ½å‡º"""
    pattern = rf'https?://[^"\s]*{re.escape(domain)}[^"\s]*'
    urls = re.findall(pattern, html_content)
    # é‡è¤‡é™¤å»ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    clean_urls = []
    seen = set()
    for url in urls:
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
        clean_url = url.split('&')[0].split('#')[0]
        if clean_url not in seen and len(clean_url) > 20:  # çŸ­ã™ãã‚‹URLã‚’é™¤å¤–
            seen.add(clean_url)
            clean_urls.append(clean_url)
    return clean_urls[:5]  # æœ€å¤§5ä»¶

def fetch_page_content(url, logger):
    """ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ï¼ˆBright DataçµŒç”± + ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    logger.log(f"ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {url}", "DEBUG")
    
    config = get_brightdata_config()
    
    # Bright DataçµŒç”±ã§è©¦è¡Œï¼ˆPOSTãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
    if config:
        try:
            logger.log(f"Bright Data POSTçµŒç”±ã§ãƒšãƒ¼ã‚¸å–å¾—è©¦è¡Œ...", "DEBUG")
            proxies = {
                'http': f"http://{config['username']}:{config['password']}@brd.superproxy.io:33335",
                'https': f"http://{config['username']}:{config['password']}@brd.superproxy.io:33335"
            }
            
            response = requests.post(
                url,
                proxies=proxies,
                timeout=10,
                headers={'User-Agent': 'Mozilla/5.0'}
            )
            
            if response.status_code == 200:
                logger.log(f"âœ“ ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸï¼ˆBright Data POSTï¼‰", "DEBUG")
                return response.text
        except Exception as e:
            logger.log(f"Bright Data POSTå¤±æ•—: {str(e)}", "WARNING")
    
    # Bright Data GET ãƒ¡ã‚½ãƒƒãƒ‰ã§è©¦è¡Œ
    if config:
        try:
            logger.log(f"Bright Data GETçµŒç”±ã§ãƒšãƒ¼ã‚¸å–å¾—è©¦è¡Œ...", "DEBUG")
            response = requests.get(
                url,
                proxies=proxies,
                timeout=10,
                headers={'User-Agent': 'Mozilla/5.0'}
            )
            
            if response.status_code == 200:
                logger.log(f"âœ“ ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸï¼ˆBright Data GETï¼‰", "DEBUG")
                return response.text
        except Exception as e:
            logger.log(f"Bright Data GETå¤±æ•—: {str(e)}", "WARNING")
    
    # ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    try:
        logger.log(f"ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãƒšãƒ¼ã‚¸å–å¾—è©¦è¡Œ...", "DEBUG")
        response = requests.get(
            url,
            timeout=10,
            headers={'User-Agent': 'Mozilla/5.0'}
        )
        
        if response.status_code == 200:
            logger.log(f"âœ“ ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸï¼ˆç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ï¼‰", "DEBUG")
            return response.text
    except Exception as e:
        logger.log(f"ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {str(e)}", "ERROR")
    
    return None

def extract_product_info_with_gemini(html_content, product_name, model, logger):
    """Gemini APIã§è£½å“æƒ…å ±ã‚’æŠ½å‡º"""
    logger.log(f"Gemini APIã§æƒ…å ±æŠ½å‡ºä¸­...", "DEBUG")
    
    try:
        # HTMLã‚’æœ€åˆã®50000æ–‡å­—ã«åˆ¶é™
        html_content = html_content[:50000]
        
        prompt = f"""
ä»¥ä¸‹ã®HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ã€åŒ–å­¦è©¦è–¬ã€Œ{product_name}ã€ã®è£½å“æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€æŠ½å‡ºã™ã‚‹æƒ…å ±ã€‘
1. productName: è£½å“åï¼ˆæ­£å¼åç§°ï¼‰
2. modelNumber: å‹ç•ªãƒ»ã‚«ã‚¿ãƒ­ã‚°ç•ªå·
3. manufacturer: ãƒ¡ãƒ¼ã‚«ãƒ¼å
4. offers: ä¾¡æ ¼æƒ…å ±ã®ãƒªã‚¹ãƒˆ
   - size: å®¹é‡ãƒ»ã‚µã‚¤ã‚º
   - price: ä¾¡æ ¼ï¼ˆæ•°å€¤ã®ã¿ã€ã‚«ãƒ³ãƒãªã—ï¼‰
   - inStock: åœ¨åº«çŠ¶æ³ï¼ˆtrue/falseï¼‰

ã€å‡ºåŠ›å½¢å¼ã€‘
å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ```jsonï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚

{{
  "productName": "è£½å“å",
  "modelNumber": "å‹ç•ª",
  "manufacturer": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
  "offers": [
    {{"size": "1 MG", "price": 34000, "inStock": true}},
    {{"size": "5 MG", "price": 130800, "inStock": true}}
  ]
}}

HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„:
{html_content}
"""
        
        response = model.generate_content(prompt)
        logger.log(f"âœ“ Gemini APIå¿œç­”å—ä¿¡", "DEBUG")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        response_text = response.text.strip()
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
        response_text = re.sub(r'^```json\s*', '', response_text)
        response_text = re.sub(r'^```\s*', '', response_text)
        response_text = re.sub(r'\s*```$', '', response_text)
        response_text = response_text.strip()
        
        # JSONè§£æ
        product_info = json.loads(response_text)
        logger.log(f"âœ“ è£½å“æƒ…å ±æŠ½å‡ºæˆåŠŸ", "INFO")
        return product_info
        
    except json.JSONDecodeError as e:
        logger.log(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        logger.log(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_text[:500]}", "DEBUG")
        return None
    except Exception as e:
        logger.log(f"æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None

def main():
    st.markdown('<h1 class="main-header">ğŸ§ª åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆæœ¬ç•ªç‰ˆï¼‰</h1>', unsafe_allow_html=True)
    
    # å¯¾è±¡ã‚µã‚¤ãƒˆè¡¨ç¤º
    st.markdown("### ğŸ“Š å¯¾è±¡ECã‚µã‚¤ãƒˆï¼ˆ11ã‚µã‚¤ãƒˆï¼‰")
    cols = st.columns(4)
    for idx, (key, site) in enumerate(TARGET_SITES.items()):
        with cols[idx % 4]:
            st.markdown(f'<span class="site-badge">{site["name"]}</span>', unsafe_allow_html=True)
    
    st.markdown("---")
    
    # è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³
    col1, col2 = st.columns([3, 1])
    
    with col1:
        product_name = st.text_input(
            "ğŸ” è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value="Y-27632",
            placeholder="ä¾‹: Y-27632, DMSO, Trizol"
        )
    
    with col2:
        max_urls = st.number_input(
            "æœ€å¤§å–å¾—URLæ•°",
            min_value=1,
            max_value=20,
            value=10,
            step=1
        )
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰
    show_debug = st.checkbox("ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’è¡¨ç¤º", value=False)
    
    st.markdown("---")
    
    # æ¤œç´¢ãƒœã‚¿ãƒ³
    if st.button("ğŸš€ æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True):
        if not product_name:
            st.warning("âš ï¸ è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        # ãƒ­ã‚°ã‚³ãƒ³ãƒ†ãƒŠ
        log_container = st.empty()
        logger = RealTimeLogger(log_container, show_debug=show_debug)
        
        # å‡¦ç†é–‹å§‹
        start_time = time.time()
        logger.log(f"å‡¦ç†é–‹å§‹: {product_name}", "INFO")
        
        # Gemini APIè¨­å®š
        model = setup_gemini()
        if not model:
            st.error("âŒ Gemini APIã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # é€²è¡ŒçŠ¶æ³
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # URLæ¤œç´¢
        status_text.text("ğŸ” è£½å“URLã‚’æ¤œç´¢ä¸­...")
        progress_bar.progress(20)
        
        urls = search_product_urls(product_name, TARGET_SITES, logger, max_urls=max_urls)
        
        if not urls:
            st.error("âŒ è£½å“URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            logger.log("æ¤œç´¢çµæœãªã—", "ERROR")
            return
        
        st.success(f"âœ… {len(urls)}ä»¶ã®URLã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
        
        # è£½å“æƒ…å ±æŠ½å‡º
        status_text.text("ğŸ“Š è£½å“æƒ…å ±ã‚’æŠ½å‡ºä¸­...")
        progress_bar.progress(40)
        
        all_products = []
        
        for idx, url_info in enumerate(urls):
            logger.log(f"å‡¦ç†ä¸­ ({idx+1}/{len(urls)}): {url_info['site']}", "INFO")
            
            # ãƒšãƒ¼ã‚¸å–å¾—
            html_content = fetch_page_content(url_info['url'], logger)
            
            if not html_content:
                logger.log(f"ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {url_info['url']}", "WARNING")
                continue
            
            # æƒ…å ±æŠ½å‡º
            product_info = extract_product_info_with_gemini(
                html_content, 
                product_name, 
                model, 
                logger
            )
            
            if product_info:
                product_info['source_site'] = url_info['site']
                product_info['source_url'] = url_info['url']
                all_products.append(product_info)
                logger.log(f"âœ“ {url_info['site']}ã‹ã‚‰æƒ…å ±æŠ½å‡ºæˆåŠŸ", "INFO")
            
            # é€²æ—æ›´æ–°
            progress = 40 + int((idx + 1) / len(urls) * 50)
            progress_bar.progress(progress)
            
            time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        progress_bar.progress(100)
        status_text.text("âœ… å‡¦ç†å®Œäº†")
        
        # å®Ÿè¡Œæ™‚é–“
        elapsed_time = time.time() - start_time
        logger.log(f"å‡¦ç†å®Œäº†: {elapsed_time:.1f}ç§’", "INFO")
        
        # çµæœè¡¨ç¤º
        st.markdown("---")
        st.markdown("## ğŸ“‹ æ¤œç´¢çµæœ")
        
        if not all_products:
            st.warning("âš ï¸ è£½å“æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        st.success(f"âœ… {len(all_products)}ä»¶ã®è£½å“æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
        
        # è£½å“æƒ…å ±è¡¨ç¤º
        for idx, product in enumerate(all_products):
            with st.expander(f"ğŸ“¦ {product.get('productName', 'ä¸æ˜')} - {product.get('source_site', 'ä¸æ˜')}", expanded=True):
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.markdown(f"**è£½å“å:** {product.get('productName', 'N/A')}")
                    st.markdown(f"**å‹ç•ª:** {product.get('modelNumber', 'N/A')}")
                    st.markdown(f"**ãƒ¡ãƒ¼ã‚«ãƒ¼:** {product.get('manufacturer', 'N/A')}")
                    st.markdown(f"**ã‚µã‚¤ãƒˆ:** {product.get('source_site', 'N/A')}")
                    st.markdown(f"**URL:** [{product.get('source_url', 'N/A')}]({product.get('source_url', '#')})")
                
                with col2:
                    if 'offers' in product and product['offers']:
                        st.markdown("**ä¾¡æ ¼æƒ…å ±:**")
                        for offer in product['offers']:
                            price_str = f"Â¥{offer.get('price', 0):,}" if offer.get('price') else 'N/A'
                            stock_str = "âœ… åœ¨åº«ã‚ã‚Š" if offer.get('inStock') else "âŒ åœ¨åº«ãªã—"
                            st.markdown(f"- {offer.get('size', 'N/A')}: {price_str} ({stock_str})")
        
        # CSVå‡ºåŠ›
        st.markdown("---")
        st.markdown("## ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        # DataFrameã«å¤‰æ›
        export_data = []
        for product in all_products:
            base_info = {
                'è£½å“å': product.get('productName', 'N/A'),
                'å‹ç•ª': product.get('modelNumber', 'N/A'),
                'ãƒ¡ãƒ¼ã‚«ãƒ¼': product.get('manufacturer', 'N/A'),
                'ã‚µã‚¤ãƒˆ': product.get('source_site', 'N/A'),
                'URL': product.get('source_url', 'N/A')
            }
            
            if 'offers' in product and product['offers']:
                for offer in product['offers']:
                    row = base_info.copy()
                    row['ã‚µã‚¤ã‚º'] = offer.get('size', 'N/A')
                    row['ä¾¡æ ¼'] = offer.get('price', 0)
                    row['åœ¨åº«'] = 'æœ‰' if offer.get('inStock') else 'ç„¡'
                    export_data.append(row)
            else:
                row = base_info.copy()
                row['ã‚µã‚¤ã‚º'] = 'N/A'
                row['ä¾¡æ ¼'] = 0
                row['åœ¨åº«'] = 'N/A'
                export_data.append(row)
        
        df = pd.DataFrame(export_data)
        
        # CSVç”Ÿæˆ
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        csv_data = csv_buffer.getvalue()
        
        st.download_button(
            label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=f"chemical_prices_{product_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True
        )
        
        # çµ±è¨ˆæƒ…å ±
        st.markdown("---")
        st.markdown("## ğŸ“ˆ çµ±è¨ˆæƒ…å ±")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("å–å¾—ã‚µã‚¤ãƒˆæ•°", len(all_products))
        
        with col2:
            total_offers = sum(len(p.get('offers', [])) for p in all_products)
            st.metric("ä¾¡æ ¼æƒ…å ±æ•°", total_offers)
        
        with col3:
            st.metric("å‡¦ç†æ™‚é–“", f"{elapsed_time:.1f}ç§’")
        
        with col4:
            st.metric("æ¤œç´¢URLæ•°", len(urls))

if __name__ == "__main__":
    main()

# -*- coding: utf-8 -*-
"""
è£½å“èª¿é”AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
ï¼ˆãƒãƒ³ã‚°å›é¿ãƒ»è¨ºæ–­æ©Ÿèƒ½å¼·åŒ–ç‰ˆï¼‰
"""

# ==============================================================================
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ==============================================================================
import streamlit as st
import pandas as pd
import time
import urllib.parse
import requests
from bs4 import BeautifulSoup
import json
import urllib3
import random
import logging
import sys

# è¨ºæ–­ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==============================================================================
# === Bright Data API é€£æºé–¢æ•° ===
# ==============================================================================

def get_page_content_with_brightdata(url: str, brd_username: str, brd_password: str, timeout: int = 15) -> dict:
    """
    Scraping Browserã§ç”Ÿbodyãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆãƒãƒ³ã‚°å›é¿ç‰ˆï¼‰
    """
    BRD_HOST = 'brd.superproxy.io'
    BRD_PORT = 24000
    proxy_url = f'http://{brd_username}:{brd_password}@{BRD_HOST}:{BRD_PORT}'
    proxies = {'http': proxy_url, 'https': proxy_url}
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'
    }
    result = {"url": url, "status_code": None, "content": None, "error": None}
    
    # ãƒ­ã‚°ã‚’è“„ç©ã—ã¦ã¾ã¨ã‚ã¦è¡¨ç¤ºï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°å›é¿ï¼‰
    logs = []
    logs.append(f"ğŸ” æ¥ç¶šé–‹å§‹: {url[:60]}...")
    logger.info(f"Starting connection to: {url}")
    
    # 1. Scraping Browserè©¦è¡Œ (POST) - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆçŸ­ç¸®
    payload = {
        'url': url,
        'renderJS': True,
        'waitFor': 3000,  # 5000 â†’ 3000ã«çŸ­ç¸®
        'proxy': 'residential'
    }
    
    try:
        logs.append(f"  ğŸ“¤ POSTæ¥ç¶šä¸­ (timeout: {timeout}ç§’)...")
        logger.info(f"POST request attempt - timeout: {timeout}s")
        sys.stdout.flush()  # ãƒ­ã‚°ã‚’å³åº§ã«å‡ºåŠ›
        
        response = requests.post(
            proxy_url,
            json=payload,
            headers=headers,
            proxies=proxies,
            verify=False,
            timeout=timeout
        )
        response.raise_for_status()
        logs.append(f"  âœ… POSTæˆåŠŸ (status: {response.status_code})")
        logger.info(f"POST success - status: {response.status_code}, length: {len(response.text)}")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ¤œè¨¼
        if len(response.text) < 500:
            logs.append(f"  âš ï¸ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒçŸ­ã™ãã‚‹ ({len(response.text)}æ–‡å­—) - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            logger.warning(f"Response too short: {len(response.text)} chars")
            raise ValueError("Response too short")
        
        # JSON or HTMLåˆ¤å®š
        try:
            data = response.json()
            html = data.get('content', response.text)
        except json.JSONDecodeError:
            html = response.text
        
    except Exception as e:
        logs.append(f"  âŒ POSTå¤±æ•—: {str(e)[:60]}")
        logger.error(f"POST failed: {str(e)[:100]}")
        
        # 2. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚­ã‚·GET
        full_url = f'{proxy_url}/{url}'
        try:
            logs.append(f"  ğŸ“¥ GETãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸­ (timeout: {timeout//2}ç§’)...")
            logger.info(f"GET fallback attempt - timeout: {timeout//2}s")
            sys.stdout.flush()
            
            response = requests.get(
                full_url,
                headers=headers,
                proxies=proxies,
                verify=False,
                timeout=timeout // 2
            )
            response.raise_for_status()
            logs.append(f"  âœ… GETæˆåŠŸ (status: {response.status_code})")
            logger.info(f"GET success - status: {response.status_code}, length: {len(response.text)}")
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ¤œè¨¼
            if len(response.text) < 500:
                logs.append(f"  âš ï¸ GETãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚‚çŸ­ã„ ({len(response.text)}æ–‡å­—) - ã‚¹ã‚­ãƒƒãƒ—")
                logger.warning(f"GET response also too short: {len(response.text)} chars")
                result["error"] = "Both POST and GET returned insufficient content"
                st.warning("\n".join(logs))
                return result
            
            html = response.text
            
        except Exception as e2:
            logs.append(f"  âŒ GETå¤±æ•—: {str(e2)[:60]}")
            logger.error(f"GET failed: {str(e2)[:100]}")
            result["error"] = f"POST: {str(e)[:50]}; GET: {str(e2)[:50]}"
            st.error("\n".join(logs))
            return result
    
    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    logs.append(f"  ğŸ”§ HTMLè§£æä¸­...")
    logger.info("Starting HTML parsing")
    sys.stdout.flush()
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe']):
            tag.decompose()
        
        body_text = soup.body.get_text(separator=' ', strip=True) if soup.body else soup.get_text(separator=' ', strip=True)
        result["content"] = body_text[:18000]
        result["status_code"] = 200
        
        logs.append(f"  âœ… æŠ½å‡ºå®Œäº†: {len(result['content'])}æ–‡å­—")
        logger.info(f"Extraction complete: {len(result['content'])} chars")
        st.success("\n".join(logs))
        
    except Exception as e:
        logs.append(f"  âŒ è§£æå¤±æ•—: {str(e)}")
        logger.error(f"Parsing failed: {str(e)}")
        result["error"] = str(e)
        st.error("\n".join(logs))
    
    return result


def search_product_urls_with_brightdata(query: str, api_key: str) -> list:
    """Bright Dataã®SERP APIã§Googleæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€URLãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ã€‚"""
    st.info(f"ã€Bright Dataã€‘ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã§æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡...")
    logger.info(f"SERP API search: {query}")
    
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'
    }
    google_search_url = f"https://www.google.co.jp/search?q={urllib.parse.quote(query)}&hl=ja&gl=jp&ceid=JP:ja"
    payload = {
        'zone': 'serp_api1',
        'url': google_search_url,
        'render': 'js'
    }
    
    time.sleep(random.uniform(1, 3))
    
    try:
        initial_response = requests.post(
            'https://api.brightdata.com/serp/req',
            headers=headers,
            json=payload,
            timeout=30
        )
        initial_response.raise_for_status()
        response_id = initial_response.headers.get('x-response-id')
        
        if not response_id:
            logger.warning("No response_id received from SERP API")
            return []
        
        logger.info(f"SERP response_id: {response_id}")
        result_url = f'https://api.brightdata.com/serp/get_result?response_id={response_id}'
        
        for attempt in range(15):
            time.sleep(random.uniform(2, 5))
            try:
                result_response = requests.get(result_url, headers=headers, timeout=30)
                
                if result_response.status_code == 200:
                    if not result_response.text:
                        logger.warning("Empty response from SERP API")
                        return []
                    
                    soup = BeautifulSoup(result_response.text, 'html.parser')
                    result_divs = soup.find_all('div', {'data-ved': True}) or soup.find_all('div', class_='g')
                    urls = []
                    
                    for div in result_divs:
                        a_tag = div.find('a', href=True)
                        if a_tag and a_tag.get('href') and a_tag.get('href').startswith('http') and not a_tag.get('href').startswith('https://www.google.'):
                            urls.append(a_tag.get('href'))
                    
                    unique_urls = list(dict.fromkeys(urls))[:10]
                    st.success(f"ã€Bright Dataã€‘ã€Œ{query}ã€ã‹ã‚‰{len(unique_urls)}ä»¶ã®URLã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
                    logger.info(f"Extracted {len(unique_urls)} URLs from SERP")
                    return unique_urls
                    
                elif result_response.status_code != 202:
                    logger.warning(f"Unexpected status code: {result_response.status_code}")
                    return []
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"SERP result fetch error (attempt {attempt+1}): {str(e)}")
                return []
        
        logger.warning("SERP API timeout after 15 attempts")
        return []
        
    except requests.exceptions.RequestException as e:
        logger.error(f"SERP API request failed: {str(e)}")
        return []

# ==============================================================================
# === AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–¢é€£é–¢æ•° ===
# ==============================================================================

def analyze_page_and_extract_info(page_content_result: dict, product_name: str, gemini_api_key: str, retry_count: int = 2) -> dict | None:
    """HTMLã‚’Gemini APIã«æ¸¡ã—ã€è£½å“æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
    body_text = page_content_result.get("content")
    if page_content_result.get("error") or not body_text:
        return None

    prompt = f"""
ã‚ãªãŸã¯åŒ–å­¦è©¦è–¬ECã‚µã‚¤ãƒˆã®æƒ…å ±æŠ½å‡ºã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚

ã€æŠ½å‡ºå¯¾è±¡è£½å“ã€‘
- è£½å“å: {product_name}

ã€Webãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆã€‘
{body_text}

ã€æŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
1. productName: ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯h1è¦ç´ ã®è£½å“å
2. modelNumber: å‹ç•ª/è£½å“ã‚³ãƒ¼ãƒ‰/ã‚«ã‚¿ãƒ­ã‚°ç•ªå·ï¼ˆä¾‹: ALX-270-333-M001ï¼‰
3. manufacturer: ãƒ¡ãƒ¼ã‚«ãƒ¼/ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼åï¼ˆä¾‹: ENZ, Selleckï¼‰
4. offers: ä¾¡æ ¼è¡¨ã‹ã‚‰ä»¥ä¸‹ã‚’æŠ½å‡º
   - size: å®¹é‡/è¦æ ¼ï¼ˆä¾‹: "1 mg", "5 mg", "1 MG"ï¼‰
   - price: ä¾¡æ ¼ã®æ•°å€¤ã®ã¿ï¼ˆä¾‹: Â¥34,000 â†’ 34000ï¼‰
   - inStock: åœ¨åº«çŠ¶æ³ï¼ˆã€Œåœ¨åº«ã‚ã‚Šã€ã€Œã‚«ãƒ¼ãƒˆã«å…¥ã‚Œã‚‹ã€ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°trueã€ãã‚Œä»¥å¤–ã¯falseï¼‰

ã€æ³¨æ„äº‹é …ã€‘
- æ–‡å­—åŒ–ã‘ã€Œï¿½ï¿½ã€ã¯ã€ŒÂ¥ã€ã¨ã—ã¦å‡¦ç†ã—ã¦ãã ã•ã„
- ä¾¡æ ¼è¡¨ãŒè¤‡æ•°è¡Œã‚ã‚‹å ´åˆã¯å…¨ã¦æŠ½å‡ºã—ã¦ãã ã•ã„
- æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„é …ç›®ã¯nullã‚’è¿”ã—ã¦ãã ã•ã„
- offersé…åˆ—ã¯å¿…ãšä½œæˆã—ã€ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ç©ºé…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„

ã€å‡ºåŠ›å½¢å¼ã€‘
{{
  "productName": "string or null",
  "modelNumber": "string or null", 
  "manufacturer": "string or null",
  "offers": [
    {{"size": "string", "price": number, "inStock": boolean}}
  ]
}}
"""
    
    for attempt in range(retry_count):
        try:
            logger.info(f"Gemini API call attempt {attempt+1}/{retry_count}")
            sys.stdout.flush()
            
            payload = {
                "contents": [{"role": "user", "parts": [{"text": prompt}]}],
                "generationConfig": {"responseMimeType": "application/json"}
            }
            api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={gemini_api_key}"
            
            response = requests.post(
                api_url,
                headers={'Content-Type': 'application/json'},
                json=payload,
                timeout=45
            )
            response.raise_for_status()
            result = response.json()
            
            if not result.get('candidates'):
                logger.warning(f"No candidates in Gemini response (attempt {attempt+1})")
                if attempt < retry_count - 1:
                    st.warning(f"âš ï¸ Geminiå¿œç­”ãªã— (è©¦è¡Œ{attempt+1}/{retry_count}) - ãƒªãƒˆãƒ©ã‚¤ä¸­...")
                    time.sleep(2)
                    continue
                return None
            
            response_text = result['candidates'][0]['content']['parts'][0]['text']
            raw_data = json.loads(response_text)
            
            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if raw_data and isinstance(raw_data, dict):
                if raw_data.get("offers") and len(raw_data["offers"]) > 0:
                    logger.info(f"Successfully extracted {len(raw_data['offers'])} offers")
                    return raw_data
                elif attempt < retry_count - 1:
                    st.warning(f"âš ï¸ offersæŠ½å‡ºå¤±æ•— (è©¦è¡Œ{attempt+1}/{retry_count}) - ãƒªãƒˆãƒ©ã‚¤ä¸­...")
                    logger.warning("No offers found in extracted data")
                    time.sleep(2)
                    continue
            
            return raw_data if isinstance(raw_data, dict) else None
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error (attempt {attempt+1}): {str(e)}")
            if attempt < retry_count - 1:
                st.warning(f"âš ï¸ JSONè§£æå¤±æ•— - ãƒªãƒˆãƒ©ã‚¤ä¸­... ({str(e)[:50]})")
                time.sleep(2)
            else:
                return None
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Gemini API request error (attempt {attempt+1}): {str(e)}")
            if attempt < retry_count - 1:
                st.warning(f"âš ï¸ APIå‘¼ã³å‡ºã—å¤±æ•— - ãƒªãƒˆãƒ©ã‚¤ä¸­... ({str(e)[:50]})")
                time.sleep(2)
            else:
                return None
    
    return None

# ==============================================================================
# === çµ±æ‹¬ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ===
# ==============================================================================

def orchestrator_agent(product_info: dict, gemini_api_key: str, brightdata_api_key: str, brd_username: str, brd_password: str, preferred_sites: list, debug_mode: bool = False) -> tuple[list, list]:
    """ä¸€é€£ã®å‡¦ç†ã‚’çµ±æ‹¬ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆãƒãƒ³ã‚°å›é¿ç‰ˆï¼‰"""
    product_name = product_info['ProductName']
    manufacturer = product_info.get('Manufacturer', '')
    st.subheader(f"ã€çµ±æ‹¬ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‘ \"{product_name}\" ã®æƒ…å ±åé›†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    logger.info(f"Orchestrator started for: {product_name} (manufacturer: {manufacturer})")

    base_query = f"{manufacturer} {product_name}"
    site_map = {
        'ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª': 'cosmobio.co.jp',
        'ãƒ•ãƒŠã‚³ã‚·': 'funakoshi.co.jp',
        'AXEL': 'axel.as-1.co.jp',
        'Selleck': 'selleck.co.jp',
        'MCE': 'medchemexpress.com',
        'Nakarai': 'nacalai.co.jp',
        'FUJIFILM': 'labchem-wako.fujifilm.com',
        'é–¢æ±åŒ–å­¦': 'kanto.co.jp',
        'TCI': 'tcichemicals.com',
        'Merck': 'merck.com',
        'å’Œå…‰ç´”è–¬': 'hpc-j.co.jp'
    }
    search_queries = [f"site:{site_map[site_name]} {base_query}" for site_name in preferred_sites if site_name in site_map]
    search_queries.append(base_query)

    # é€²æ—ãƒãƒ¼åˆæœŸåŒ– (0%)
    progress_bar = st.progress(0)
    status_text = st.empty()

    # ã‚¹ãƒ†ãƒƒãƒ—1: URLæŠ½å‡º (0-20%)
    status_text.text("URLæŠ½å‡ºä¸­...")
    progress_bar.progress(0.05)
    logger.info(f"Starting URL extraction - {len(search_queries)} queries")
    
    all_urls = []
    num_queries = len(search_queries)
    
    for i, query in enumerate(search_queries):
        urls = search_product_urls_with_brightdata(query, brightdata_api_key)
        all_urls.extend(urls)
        if urls and debug_mode:
            st.info(f"âœ… ã‚¯ã‚¨ãƒª{i+1}: {len(urls)}ä»¶å–å¾—")
        progress = 0.05 + (i / num_queries) * 0.15
        progress_bar.progress(progress)
        status_text.text(f"URLæŠ½å‡ºä¸­... ({i+1}/{num_queries})")
    
    unique_urls = list(dict.fromkeys(all_urls))[:15]  # 10 â†’ 15ã«å¢—åŠ ï¼ˆå¤±æ•—ã‚’è¦‹è¶Šã—ã¦ï¼‰
    
    if not unique_urls:
        st.error("æ¤œç´¢çµæœã‹ã‚‰URLã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        logger.error("No URLs extracted from search results")
        return [], []
    
    st.info(f"ğŸ“Š {len(unique_urls)}ä»¶ã®URLã‚’å–å¾—ã—ã¾ã—ãŸã€‚é †æ¬¡ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™...")
    logger.info(f"Extracted {len(unique_urls)} unique URLs")
    progress_bar.progress(0.2)
    status_text.text("URLæŠ½å‡ºå®Œäº† (20%)")

    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒšãƒ¼ã‚¸å–å¾— (20-70%) - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆåˆ¶å¾¡è¿½åŠ 
    status_text.text("Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
    logger.info("Starting page content retrieval")
    all_page_content_results = []
    success_count = 0
    fail_count = 0

    for i, url in enumerate(unique_urls):
        status_text.text(f"ğŸ“„ å–å¾—ä¸­ ({i + 1}/{len(unique_urls)}): {url[:50]}...")
        logger.info(f"Processing URL {i+1}/{len(unique_urls)}: {url}")
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å‹•çš„ã«èª¿æ•´ï¼ˆå¤±æ•—ãŒå¤šã„å ´åˆã¯çŸ­ãã™ã‚‹ï¼‰
        dynamic_timeout = 12 if fail_count < 3 else 8
        
        try:
            page_result = get_page_content_with_brightdata(url, brd_username, brd_password, timeout=dynamic_timeout)
            all_page_content_results.append(page_result)
            
            if page_result.get('content') and len(page_result.get('content', '')) > 1000:
                success_count += 1
                logger.info(f"Success: {url} ({len(page_result['content'])} chars)")
            else:
                fail_count += 1
                logger.warning(f"Insufficient content: {url} ({len(page_result.get('content', ''))} chars)")
                if debug_mode:
                    st.warning(f"âš ï¸ å†…å®¹ä¸è¶³: {url[:50]} ({len(page_result.get('content', ''))}æ–‡å­—)")
        
        except Exception as e:
            fail_count += 1
            logger.error(f"Unexpected error for {url}: {str(e)}")
            st.error(f"ğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {url[:50]} - {str(e)[:50]}")
            all_page_content_results.append({"url": url, "error": str(e)})
        
        # é€²æ—æ›´æ–°
        progress = 0.2 + (i + 1) / len(unique_urls) * 0.5
        progress_bar.progress(progress)
        
        # æ—©æœŸçµ‚äº†åˆ¤å®šï¼ˆæˆåŠŸãŒ5ä»¶ä»¥ä¸Šã‚ã‚Œã°æ®‹ã‚Šã‚’ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½ï¼‰
        if success_count >= 5 and i >= 10:
            st.info(f"âœ… ååˆ†ãªæƒ…å ±ã‚’å–å¾— ({success_count}ä»¶æˆåŠŸ) - æ®‹ã‚Šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            logger.info(f"Early termination: {success_count} successes achieved")
            break

    st.info(f"ğŸ“ˆ å–å¾—çµæœ: æˆåŠŸ {success_count}ä»¶ / å¤±æ•— {fail_count}ä»¶")
    logger.info(f"Page retrieval complete - Success: {success_count}, Fail: {fail_count}")
    progress_bar.progress(0.7)
    status_text.text("ãƒšãƒ¼ã‚¸å–å¾—å®Œäº† (70%)")

    # ã‚¹ãƒ†ãƒƒãƒ—3: AIè§£æ (70-100%)
    status_text.text("AIã§ãƒšãƒ¼ã‚¸ã‚’åˆ†æä¸­...")
    logger.info("Starting AI analysis")
    found_pages_data = []
    successful_contents = [
        res for res in all_page_content_results 
        if res.get("content") and len(res.get("content", "")) > 1000 and not res.get("error")
    ]
    
    if not successful_contents:
        st.warning("æœ‰åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        logger.warning("No valid content for AI analysis")
        progress_bar.progress(1.0)
        status_text.text("å®Œäº† (100%) - ãƒ‡ãƒ¼ã‚¿ãªã—")
        return [], all_page_content_results
    
    logger.info(f"Analyzing {len(successful_contents)} pages with AI")
    
    for i, content_res in enumerate(successful_contents):
        status_text.text(f"ğŸ¤– AIåˆ†æä¸­... ({i + 1}/{len(successful_contents)})")
        
        try:
            page_details = analyze_page_and_extract_info(content_res, product_name, gemini_api_key)
            
            if page_details and page_details.get("offers"):
                page_details['sourceUrl'] = content_res.get("url")
                found_pages_data.append(page_details)
                logger.info(f"Extracted {len(page_details['offers'])} offers from {content_res.get('url')}")
                if debug_mode:
                    st.success(f"âœ… æŠ½å‡ºæˆåŠŸ: {len(page_details['offers'])}ä»¶ã®offer")
                    with st.expander(f"AIè§£æçµæœ: {content_res.get('url')[:50]}"):
                        st.json(page_details)
        
        except Exception as e:
            logger.error(f"AI analysis error: {str(e)}")
            st.warning(f"âš ï¸ AIè§£æã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
        
        progress = 0.7 + (i + 1) / len(successful_contents) * 0.3
        progress_bar.progress(progress)

    progress_bar.progress(1.0)
    status_text.text("å®Œäº† (100%)")
    st.success(f"ã€çµ±æ‹¬ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‘{len(found_pages_data)}ãƒšãƒ¼ã‚¸ã‹ã‚‰è£½å“æƒ…å ±ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
    logger.info(f"Orchestrator complete - {len(found_pages_data)} pages with product info")
    
    return found_pages_data, all_page_content_results

# ==============================================================================
# === Streamlit UI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³éƒ¨åˆ† ===
# ==============================================================================

st.set_page_config(layout="wide")
st.title("è£½å“èª¿é”AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")

st.sidebar.header("APIã‚­ãƒ¼è¨­å®š")
try:
    gemini_api_key = st.secrets["GOOGLE_API_KEY"]
    brightdata_api_key = st.secrets["BRIGHTDATA_API_KEY"]
    brightdata_username = st.secrets["BRIGHTDATA_USERNAME"]
    brightdata_password = st.secrets["BRIGHTDATA_PASSWORD"]
    st.sidebar.success("âœ… APIã‚­ãƒ¼ã¨èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚")
    logger.info("API credentials loaded successfully")
except KeyError as e:
    st.sidebar.error("âŒ Streamlit Secretsã«å¿…è¦ãªæƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    logger.error(f"Missing secret key: {str(e)}")
    gemini_api_key, brightdata_api_key, brightdata_username, brightdata_password = "", "", "", ""

st.sidebar.header("æ¤œç´¢æ¡ä»¶")
product_name_input = st.sidebar.text_input("è£½å“å (å¿…é ˆ)", placeholder="ä¾‹: Y27632")
manufacturer_input = st.sidebar.text_input("ãƒ¡ãƒ¼ã‚«ãƒ¼", placeholder="ä¾‹: Selleck")
min_price_input = st.sidebar.number_input("æœ€ä½ä¾¡æ ¼ (å††)", min_value=0, value=0, step=100)
max_price_input = st.sidebar.number_input("æœ€é«˜ä¾¡æ ¼ (å††)", min_value=0, value=0, step=100)
debug_mode_checkbox = st.sidebar.checkbox("ğŸ”§ ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ (è©³ç´°ãƒ­ã‚°è¡¨ç¤º)")
search_button = st.sidebar.button("ğŸš€ æ¤œç´¢é–‹å§‹", type="primary")

if search_button:
    if not all([gemini_api_key, brightdata_api_key, brightdata_username, brightdata_password]):
        st.error("âŒ APIã‚­ãƒ¼ã¾ãŸã¯èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        logger.error("API credentials not configured")
    elif not product_name_input:
        st.error("âŒ è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        logger.error("Product name not provided")
    else:
        logger.info(f"Search started - Product: {product_name_input}, Manufacturer: {manufacturer_input}")
        
        with st.spinner('AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæƒ…å ±åé›†ä¸­...'):
            product_info = {
                'ProductName': product_name_input,
                'Manufacturer': manufacturer_input
            }
            preferred_sites = [
                'ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª', 'ãƒ•ãƒŠã‚³ã‚·', 'AXEL', 'Selleck', 'MCE',
                'Nakarai', 'FUJIFILM', 'é–¢æ±åŒ–å­¦', 'TCI', 'Merck', 'å’Œå…‰ç´”è–¬'
            ]
            
            pages_list, log_data = orchestrator_agent(
                product_info,
                gemini_api_key,
                brightdata_api_key,
                brightdata_username,
                brightdata_password,
                preferred_sites,
                debug_mode=debug_mode_checkbox
            )
            
            final_results = []
            input_date = pd.Timestamp.now().strftime('%Y-%m-%d')
            
            if pages_list:
                for page_data in pages_list:
                    for offer_item in page_data.get('offers', []):
                        try:
                            price = int(float(offer_item.get('price', 0)))
                        except (ValueError, TypeError):
                            price = 0
                        
                        final_results.append({
                            'å…¥åŠ›æ—¥': input_date,
                            'è£½å“å': page_data.get('productName', 'N/A'),
                            'å‹ç•ª/è£½å“ç•ªå·': page_data.get('modelNumber', 'N/A'),
                            'ä»•æ§˜': offer_item.get('size', 'N/A'),
                            'ãƒ¡ãƒ¼ã‚«ãƒ¼': page_data.get('manufacturer', 'N/A'),
                            'ãƒªã‚¹ãƒˆå˜ä¾¡': price,
                            'åœ¨åº«': 'ã‚ã‚Š' if offer_item.get('inStock') else 'ãªã—/ä¸æ˜',
                            'æƒ…å ±å…ƒURL': page_data.get('sourceUrl', 'N/A')
                        })
            
            if not final_results:
                st.warning("âš ï¸ æ¤œç´¢çµæœã‹ã‚‰æœ‰åŠ¹ãªè£½å“æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                logger.warning("No product information extracted")
                search_term = f"{product_info.get('Manufacturer', '')} {product_info['ProductName']}"
                query_url = f"https://www.google.com/search?q={urllib.parse.quote(search_term)}"
                final_results.append({
                    'å…¥åŠ›æ—¥': input_date,
                    'è£½å“å': product_info['ProductName'],
                    'å‹ç•ª/è£½å“ç•ªå·': 'N/A',
                    'ä»•æ§˜': 'N/A',
                    'ãƒ¡ãƒ¼ã‚«ãƒ¼': product_info.get('Manufacturer', ''),
                    'ãƒªã‚¹ãƒˆå˜ä¾¡': 0,
                    'åœ¨åº«': 'ãªã—/ä¸æ˜',
                    'æƒ…å ±å…ƒURL': query_url
                })
            
            st.success("âœ… å…¨è£½å“ã®æƒ…å ±åé›†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            logger.info(f"Search complete - {len(final_results)} results")

            df_results = pd.DataFrame(final_results)
            
            # ä¾¡æ ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if max_price_input > 0:
                df_results = df_results[df_results['ãƒªã‚¹ãƒˆå˜ä¾¡'] <= max_price_input]
            if min_price_input > 0:
                df_results = df_results[df_results['ãƒªã‚¹ãƒˆå˜ä¾¡'] >= min_price_input]

            st.subheader("ğŸ“Š æ¤œç´¢çµæœ")
            st.dataframe(
                df_results,
                column_config={
                    "ãƒªã‚¹ãƒˆå˜ä¾¡": st.column_config.NumberColumn(format="Â¥%d"),
                    "æƒ…å ±å…ƒURL": st.column_config.LinkColumn("Link", display_text="é–‹ã")
                },
                use_container_width=True,
                hide_index=True
            )
            
            @st.cache_data
            def convert_df_to_csv(df: pd.DataFrame) -> bytes:
                return df.to_csv(index=False).encode('utf-8-sig')
            
            csv = convert_df_to_csv(df_results)
            st.download_button(
                label="ğŸ“¥ çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name=f"purchase_list_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime='text/csv'
            )

        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: è©³ç´°ãƒ­ã‚°è¡¨ç¤º
        if debug_mode_checkbox and log_data:
            st.subheader("ğŸ” è©³ç´°ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°")
            logger.info("Displaying debug logs")
            
            for idx, log in enumerate(log_data):
                status = log.get('status_code')
                is_error = log.get('error') is not None
                
                if is_error:
                    st.error(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {log['url']}")
                elif status != 200 and status is not None:
                    st.warning(f"âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ç•°å¸¸ ({status}): {log['url']}")
                else:
                    st.success(f"âœ… å–å¾—æˆåŠŸ ({status}): {log['url']}")

                with st.expander(f"è©³ç´°ã‚’è¡¨ç¤º - URL {idx+1}"):
                    if is_error:
                        st.write(f"**ã‚¨ãƒ©ãƒ¼å†…å®¹:** `{log['error']}`")
                    
                    log_display = log.copy()
                    if log_display.get('content'):
                        content_len = len(log_display['content'])
                        log_display['content'] = (
                            log_display['content'][:1000] + "..."
                            if len(log_display['content']) > 1000
                            else log_display['content']
                        )
                        st.write(f"**Contenté•·:** {content_len}æ–‡å­—")
                        if content_len < 1000:
                            st.warning("âš ï¸ ã“ã®ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒçŸ­ã™ãã¾ã™ï¼ˆãƒ–ãƒ­ãƒƒã‚¯ç–‘ã„ï¼‰ã€‚")
                    
                    st.json(log_display)

logger.info("Application ready")

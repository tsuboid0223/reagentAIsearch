import streamlit as st
import requests
import google.generativeai as genai
import time
import re
import json
import pandas as pd
from urllib.parse import quote_plus
from io import StringIO
from datetime import datetime

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆSERP APIç‰ˆï¼‰",
    page_icon="ğŸ§ª",
    layout="wide"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .product-card {
        background-color: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 0.5rem;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    .product-title {
        font-size: 1.3rem;
        font-weight: bold;
        color: #2c3e50;
        margin-bottom: 0.5rem;
    }
    .product-info {
        font-size: 1rem;
        color: #495057;
        margin: 0.3rem 0;
    }
    .price-row {
        background-color: white;
        padding: 0.8rem;
        margin: 0.3rem 0;
        border-radius: 0.3rem;
        border-left: 4px solid #007bff;
    }
    .log-container {
        background-color: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 0.5rem;
        padding: 1rem;
        font-family: monospace;
        font-size: 0.85rem;
        max-height: 400px;
        overflow-y: auto;
        margin: 1rem 0;
    }
    .api-status {
        padding: 0.5rem 1rem;
        border-radius: 0.3rem;
        margin: 0.5rem 0;
        font-weight: bold;
    }
    .api-success {
        background-color: #d4edda;
        color: #155724;
        border: 1px solid #c3e6cb;
    }
    .api-warning {
        background-color: #fff3cd;
        color: #856404;
        border: 1px solid #ffeeba;
    }
</style>
""", unsafe_allow_html=True)

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ã‚¯ãƒ©ã‚¹
class RealTimeLogger:
    def __init__(self, container):
        self.container = container
        self.logs = []
        
    def log(self, message, level="INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"
        self.logs.append(log_entry)
        
        with self.container:
            st.code("\n".join(self.logs[-30:]), language="log")

# Gemini APIè¨­å®š
def setup_gemini():
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        return genai.GenerativeModel('gemini-2.0-flash')
    except Exception as e:
        st.error(f"âŒ Gemini APIè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

# SERP APIè¨­å®šãƒã‚§ãƒƒã‚¯
def check_serp_api_config():
    """SERP APIèªè¨¼æƒ…å ±ã®ç¢ºèª"""
    try:
        if "BRIGHTDATA_API_KEY" in st.secrets:
            zone_name = st.secrets.get("BRIGHTDATA_ZONE_NAME", "serp_api1")
            return {
                'provider': 'brightdata',
                'auth_type': 'api_key',
                'api_key': st.secrets["BRIGHTDATA_API_KEY"],
                'zone_name': zone_name,
                'available': True
            }
    except:
        pass
    
    return {
        'provider': None,
        'available': False
    }

# å¯¾è±¡ECã‚µã‚¤ãƒˆã®å®šç¾©ï¼ˆ11ã‚µã‚¤ãƒˆï¼‰
TARGET_SITES = {
    "cosmobio": {"name": "ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª", "domain": "cosmobio.co.jp"},
    "funakoshi": {"name": "ãƒ•ãƒŠã‚³ã‚·", "domain": "funakoshi.co.jp"},
    "axel": {"name": "AXEL", "domain": "axel.as-1.co.jp"},
    "selleck": {"name": "Selleck", "domain": "selleck.co.jp"},
    "mce": {"name": "MCE", "domain": "medchemexpress.com"},
    "nakarai": {"name": "ãƒŠã‚«ãƒ©ã‚¤", "domain": "nacalai.co.jp"},
    "fujifilm": {"name": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ å’Œå…‰", "domain": "labchem-wako.fujifilm.com"},
    "kanto": {"name": "é–¢æ±åŒ–å­¦", "domain": "kanto.co.jp"},
    "tci": {"name": "TCI", "domain": "tcichemicals.com"},
    "merck": {"name": "Merck", "domain": "merck.com"},
    "wako": {"name": "å’Œå…‰ç´”è–¬", "domain": "hpc-j.co.jp"}
}

def search_with_brightdata_serp(query, serp_config, logger):
    """Bright Data SERP APIã§æ¤œç´¢"""
    try:
        logger.log(f"  ğŸ”Œ Bright Data SERP APIä½¿ç”¨", "DEBUG")
        logger.log(f"  ğŸ”‘ APIã‚­ãƒ¼èªè¨¼ã‚’ä½¿ç”¨", "DEBUG")
        
        api_url = "https://api.brightdata.com/request"
        search_url = f"https://www.google.com/search?q={quote_plus(query)}&num=10&hl=ja&gl=jp"
        
        headers = {
            'Authorization': f'Bearer {serp_config["api_key"]}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'zone': serp_config['zone_name'],
            'url': search_url,
            'format': 'raw'
        }
        
        logger.log(f"  ğŸ“¡ æ¤œç´¢URL: {search_url[:80]}...", "DEBUG")
        
        response = requests.post(api_url, headers=headers, json=payload, timeout=30)
        
        if response.status_code == 200:
            logger.log(f"  âœ“ SERP APIå¿œç­”æˆåŠŸ", "DEBUG")
            return {'html': response.text, 'status': 'success'}
        elif response.status_code == 401:
            logger.log(f"  âŒ èªè¨¼ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„", "ERROR")
            return None
        elif response.status_code == 429:
            logger.log(f"  âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«åˆ°é”", "WARNING")
            return None
        else:
            logger.log(f"  âš ï¸ SERP API ã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}", "WARNING")
            logger.log(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text[:200]}", "DEBUG")
            return None
            
    except requests.exceptions.Timeout:
        logger.log(f"  â±ï¸ SERP APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ", "WARNING")
        return None
    except Exception as e:
        logger.log(f"  âŒ SERP APIã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None

def extract_urls_from_html(html_content, domain, logger):
    """HTMLã‹ã‚‰URLã‚’æŠ½å‡º"""
    urls = []
    
    try:
        patterns = [
            rf'https?://(?:www\.)?{re.escape(domain)}[^\s<>"\']*',
            rf'href="(https?://(?:www\.)?{re.escape(domain)}[^"]*)"',
            rf"href='(https?://(?:www\.)?{re.escape(domain)}[^']*)'",
        ]
        
        all_urls = set()
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                url = match[0] if isinstance(match, tuple) else match
                if url.startswith('http') and len(url) > 20:
                    all_urls.add(url)
        
        for url in list(all_urls)[:10]:
            if any(x in url.lower() for x in ['google.com', 'youtube.com', 'facebook.com']):
                continue
                
            urls.append({
                'url': url,
                'title': '',
                'snippet': ''
            })
        
        if urls:
            logger.log(f"  âœ“ HTMLã‹ã‚‰{len(urls)}ä»¶ã®URLæŠ½å‡º", "INFO")
        else:
            logger.log(f"  â„¹ï¸ è©²å½“URLãªã—", "DEBUG")
        
        return urls
        
    except Exception as e:
        logger.log(f"  URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}", "WARNING")
        return []

def search_with_strategy(product_name, site_info, serp_config, logger):
    """SERP APIã‚’ä½¿ç”¨ã—ãŸæ¤œç´¢æˆ¦ç•¥"""
    site_name = site_info["name"]
    domain = site_info["domain"]
    
    logger.log(f"ğŸ” {site_name}ã‚’æ¤œç´¢ä¸­", "INFO")
    
    if not serp_config['available']:
        logger.log(f"  âŒ SERP APIæœªè¨­å®š", "ERROR")
        return []
    
    search_queries = [
        f"{product_name} ä¾¡æ ¼ site:{domain}",
        f"{product_name} site:{domain}",
        f"{product_name} ã‚«ã‚¿ãƒ­ã‚° site:{domain}",
    ]
    
    all_results = []
    
    for query_idx, query in enumerate(search_queries):
        logger.log(f"  æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³{query_idx+1}: {query[:60]}...", "DEBUG")
        
        serp_data = search_with_brightdata_serp(query, serp_config, logger)
        
        if not serp_data:
            logger.log(f"  âš ï¸ SERP APIå¿œç­”ãªã—ã€æ¬¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¸", "DEBUG")
            time.sleep(2)
            continue
        
        urls = extract_urls_from_html(serp_data['html'], domain, logger)
        
        if urls:
            for url_data in urls[:5]:
                all_results.append({
                    'url': url_data['url'],
                    'site': site_name,
                    'title': url_data.get('title', ''),
                    'snippet': url_data.get('snippet', ''),
                    'html': serp_data['html']
                })
            
            logger.log(f"  âœ… {len(urls)}ä»¶ã®URLå–å¾—æˆåŠŸ", "INFO")
            break
        
        time.sleep(2)
    
    if all_results:
        logger.log(f"âœ… {site_name}: {len(all_results)}ä»¶ã®URLå–å¾—", "INFO")
    else:
        logger.log(f"âš ï¸ {site_name}: URLæœªç™ºè¦‹", "WARNING")
    
    return all_results

def fetch_page_content(url, logger):
    """ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8',
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            logger.log(f"  âœ“ ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ", "DEBUG")
            return response.text
        else:
            logger.log(f"  âš ï¸ ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: HTTP {response.status_code}", "DEBUG")
            return None
    except Exception as e:
        logger.log(f"  ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}", "DEBUG")
        return None

def extract_product_info_from_page(html_content, product_name, model, logger):
    """ãƒšãƒ¼ã‚¸HTMLã‹ã‚‰è£½å“æƒ…å ±ã‚’æŠ½å‡º"""
    logger.log(f"  ğŸ¤– ãƒšãƒ¼ã‚¸å†…å®¹ã‚’AIåˆ†æä¸­", "DEBUG")
    
    try:
        html_content = html_content[:50000]
        
        prompt = f"""
ä»¥ä¸‹ã®HTMLã‹ã‚‰åŒ–å­¦è©¦è–¬ã€Œ{product_name}ã€ã®è£½å“æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€æŠ½å‡ºæƒ…å ±ã€‘
1. productName: è£½å“åï¼ˆæ–‡å­—åˆ—ï¼‰
2. modelNumber: å‹ç•ªï¼ˆæ–‡å­—åˆ—ï¼‰
3. manufacturer: ãƒ¡ãƒ¼ã‚«ãƒ¼åï¼ˆæ–‡å­—åˆ—ï¼‰
4. offers: ä¾¡æ ¼æƒ…å ±ã®é…åˆ—
   å„è¦ç´ :
   - size: å®¹é‡ãƒ»ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—åˆ—ï¼‰
   - price: ä¾¡æ ¼ï¼ˆæ•°å€¤å‹ã€ã‚«ãƒ³ãƒãªã—ï¼‰
   - inStock: åœ¨åº«çŠ¶æ³ï¼ˆçœŸå½å€¤ï¼‰

ã€é‡è¦ã€‘
- priceã¯å¿…ãšæ•°å€¤å‹ï¼ˆæ•´æ•°ã¾ãŸã¯å°æ•°ï¼‰ã§è¿”ã™
- ä¾¡æ ¼ãŒãªã„å ´åˆã¯offersã‚’ç©ºé…åˆ—[]ã«ã™ã‚‹
- æ–‡å­—åˆ—ã¯å¼•ç”¨ç¬¦ã§å›²ã‚€

JSONå½¢å¼ã§å‡ºåŠ›:
{{
  "productName": "è£½å“å",
  "modelNumber": "å‹ç•ª",
  "manufacturer": "ãƒ¡ãƒ¼ã‚«ãƒ¼",
  "offers": [
    {{"size": "10mg", "price": 12000, "inStock": true}}
  ]
}}

HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„:
{html_content}
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        response_text = re.sub(r'^```json\s*', '', response_text)
        response_text = re.sub(r'^```\s*', '', response_text)
        response_text = re.sub(r'\s*```$', '', response_text)
        response_text = response_text.strip()
        
        product_info = json.loads(response_text)
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼ã¨ä¿®æ­£
        if 'offers' in product_info and isinstance(product_info['offers'], list):
            for offer in product_info['offers']:
                if 'price' in offer:
                    # ä¾¡æ ¼ã‚’æ•°å€¤ã«å¤‰æ›
                    try:
                        if isinstance(offer['price'], str):
                            # ã‚«ãƒ³ãƒã‚’å‰Šé™¤ã—ã¦æ•°å€¤åŒ–
                            offer['price'] = float(offer['price'].replace(',', '').replace('Â¥', '').replace('å††', '').strip())
                        else:
                            offer['price'] = float(offer['price'])
                    except:
                        offer['price'] = 0
        
        if product_info.get('offers'):
            logger.log(f"  âœ… ãƒšãƒ¼ã‚¸ã‹ã‚‰{len(product_info['offers'])}ä»¶ã®ä¾¡æ ¼æŠ½å‡º", "INFO")
        else:
            logger.log(f"  â„¹ï¸ ãƒšãƒ¼ã‚¸ã«ä¾¡æ ¼æƒ…å ±ãªã—", "DEBUG")
        
        return product_info
        
    except Exception as e:
        logger.log(f"  ãƒšãƒ¼ã‚¸è§£æã‚¨ãƒ©ãƒ¼: {str(e)}", "DEBUG")
        return None

def display_product_card(product, idx):
    """è£½å“æƒ…å ±ã‚’è¡¨ç¤º"""
    st.markdown(f'<div class="product-card">', unsafe_allow_html=True)
    
    product_name = product.get('productName', 'è£½å“åä¸æ˜')
    site_name = product.get('source_site', 'ä¸æ˜')
    st.markdown(f'<div class="product-title">ğŸ“¦ {product_name}</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown(f'<div class="product-info"><strong>è²©å£²å…ƒ:</strong> {site_name}</div>', unsafe_allow_html=True)
        st.markdown(f'<div class="product-info"><strong>å‹ç•ª:</strong> {product.get("modelNumber", "N/A")}</div>', unsafe_allow_html=True)
        st.markdown(f'<div class="product-info"><strong>ãƒ¡ãƒ¼ã‚«ãƒ¼:</strong> {product.get("manufacturer", "N/A")}</div>', unsafe_allow_html=True)
    
    with col2:
        source_url = product.get('source_url', '#')
        st.markdown(f'<div class="product-info"><strong>URL:</strong> <a href="{source_url}" target="_blank">è£½å“ãƒšãƒ¼ã‚¸ã‚’é–‹ã</a></div>', unsafe_allow_html=True)
    
    if 'offers' in product and product['offers']:
        st.markdown("**ğŸ’° ä¾¡æ ¼æƒ…å ±:**")
        
        for offer in product['offers']:
            size = offer.get('size', 'N/A')
            price = offer.get('price', 0)
            
            # ä¾¡æ ¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¿®æ­£
            try:
                if isinstance(price, (int, float)) and price > 0:
                    price_str = f"Â¥{int(price):,}"
                else:
                    price_str = 'N/A'
            except:
                price_str = 'N/A'
            
            stock = offer.get('inStock', False)
            stock_icon = "âœ…" if stock else "âŒ"
            stock_text = "åœ¨åº«ã‚ã‚Š" if stock else "åœ¨åº«ãªã—"
            
            st.markdown(
                f'<div class="price-row">'
                f'<strong>{size}</strong>: {price_str} {stock_icon} {stock_text}'
                f'</div>',
                unsafe_allow_html=True
            )
    else:
        st.warning("âš ï¸ ä¾¡æ ¼æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    st.markdown('</div>', unsafe_allow_html=True)

def main():
    st.markdown('<h1 class="main-header">ğŸ§ª åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆSERP APIç‰ˆ Finalï¼‰</h1>', unsafe_allow_html=True)
    
    serp_config = check_serp_api_config()
    
    if serp_config['available']:
        st.markdown(
            f'<div class="api-status api-success">âœ… SERP APIæ¥ç¶š: BRIGHTDATA (Zone: {serp_config["zone_name"]})</div>',
            unsafe_allow_html=True
        )
    else:
        st.markdown(
            '<div class="api-status api-warning">âš ï¸ SERP APIæœªè¨­å®š: secrets.tomlã«BRIGHTDATA_API_KEYã‚’è¿½åŠ ã—ã¦ãã ã•ã„</div>',
            unsafe_allow_html=True
        )
        st.info("""
        **SERP APIè¨­å®šæ–¹æ³•:**
        
        `.streamlit/secrets.toml`ã«ä»¥ä¸‹ã‚’è¿½åŠ :
        ```toml
        BRIGHTDATA_API_KEY = "your_api_key"
        BRIGHTDATA_ZONE_NAME = "serp_api1"
        ```
        """)
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        product_name = st.text_input(
            "ğŸ” è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value="Y-27632",
            placeholder="ä¾‹: Y-27632, DMSO, Trizol, Quinpirole"
        )
    
    with col2:
        max_sites = st.number_input(
            "æœ€å¤§æ¤œç´¢ã‚µã‚¤ãƒˆæ•°",
            min_value=1,
            max_value=11,
            value=3,
            step=1
        )
    
    st.markdown("---")
    
    search_disabled = not serp_config['available']
    
    if st.button("ğŸš€ æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True, disabled=search_disabled):
        if not product_name:
            st.warning("âš ï¸ è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        st.markdown("### ğŸ“ å‡¦ç†ãƒ­ã‚°")
        log_container = st.empty()
        logger = RealTimeLogger(log_container)
        
        start_time = time.time()
        logger.log(f"ğŸš€ å‡¦ç†é–‹å§‹: {product_name}", "INFO")
        logger.log(f"ğŸ”Œ SERP API: BRIGHTDATA (Zone: {serp_config['zone_name']})", "INFO")
        
        model = setup_gemini()
        if not model:
            st.error("âŒ Gemini APIã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        all_products = []
        sites_to_search = dict(list(TARGET_SITES.items())[:max_sites])
        
        for site_key, site_info in sites_to_search.items():
            search_results = search_with_strategy(product_name, site_info, serp_config, logger)
            
            if not search_results:
                time.sleep(2)
                continue
            
            # æœ€åˆã®URLã‚’ä½¿ç”¨
            result = search_results[0]
            
            # ãƒšãƒ¼ã‚¸åˆ†æã®ã¿ï¼ˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã¯ç¾åœ¨ç©ºãªã®ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            page_info = None
            html_content = fetch_page_content(result['url'], logger)
            if html_content:
                page_info = extract_product_info_from_page(html_content, product_name, model, logger)
            
            if page_info:
                page_info['source_site'] = result['site']
                page_info['source_url'] = result['url']
                all_products.append(page_info)
                logger.log(f"âœ… {result['site']}: è£½å“æƒ…å ±å–å¾—æˆåŠŸ", "INFO")
            else:
                logger.log(f"âš ï¸ {result['site']}: è£½å“æƒ…å ±å–å¾—å¤±æ•—", "WARNING")
            
            time.sleep(3)
        
        elapsed_time = time.time() - start_time
        logger.log(f"ğŸ‰ å‡¦ç†å®Œäº†: {elapsed_time:.1f}ç§’", "INFO")
        
        st.markdown("---")
        st.markdown("## ğŸ“‹ æ¤œç´¢çµæœ")
        
        if not all_products:
            st.warning("âš ï¸ è£½å“æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        with_price = [p for p in all_products if p.get('offers')]
        without_price = [p for p in all_products if not p.get('offers')]
        
        st.success(f"âœ… {len(all_products)}ä»¶ã®è£½å“æƒ…å ±ã‚’å–å¾—ï¼ˆä¾¡æ ¼æƒ…å ±ã‚ã‚Š: {len(with_price)}ä»¶ã€å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’ï¼‰")
        
        for idx, product in enumerate(with_price + without_price):
            display_product_card(product, idx)
        
        # CSVå‡ºåŠ›
        st.markdown("---")
        st.markdown("## ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        export_data = []
        for product in all_products:
            base_info = {
                'è£½å“å': product.get('productName', 'N/A'),
                'å‹ç•ª': product.get('modelNumber', 'N/A'),
                'ãƒ¡ãƒ¼ã‚«ãƒ¼': product.get('manufacturer', 'N/A'),
                'è²©å£²å…ƒ': product.get('source_site', 'N/A'),
                'URL': product.get('source_url', 'N/A')
            }
            
            if 'offers' in product and product['offers']:
                for offer in product['offers']:
                    row = base_info.copy()
                    row['ã‚µã‚¤ã‚º'] = offer.get('size', 'N/A')
                    
                    # ä¾¡æ ¼ã®å®‰å…¨ãªå‡¦ç†
                    try:
                        price = offer.get('price', 0)
                        row['ä¾¡æ ¼'] = int(price) if isinstance(price, (int, float)) else 0
                    except:
                        row['ä¾¡æ ¼'] = 0
                    
                    row['åœ¨åº«'] = 'æœ‰' if offer.get('inStock') else 'ç„¡'
                    export_data.append(row)
            else:
                row = base_info.copy()
                row['ã‚µã‚¤ã‚º'] = 'N/A'
                row['ä¾¡æ ¼'] = 0
                row['åœ¨åº«'] = 'N/A'
                export_data.append(row)
        
        df = pd.DataFrame(export_data)
        
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        csv_data = csv_buffer.getvalue()
        
        st.download_button(
            label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=f"chemical_prices_{product_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True
        )

if __name__ == "__main__":
    main()
